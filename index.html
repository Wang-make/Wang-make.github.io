<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="This is myblog!"><meta name="keywords" content><meta name="author" content="bb,shuiyue75381@gmail.com"><meta name="copyright" content="bb"><title>【bb的博客】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!--link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!--link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!--script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"X7RY6NGU4G","apiKey":"bf7ed65264918bbc03f12b4cc1212d85","indexName":"myblog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">bb</div><div class="author-info-description">This is myblog!</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/bbkali" target="_blank">GitHub<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="mailto:shuiyue75381@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color9"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">78</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">64</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">30</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">bb的博客</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-Spider详解/">scrapy-Spider详解</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程03-Spider详解"><a href="#Scrapy教程03-Spider详解" class="headerlink" title="Scrapy教程03- Spider详解"></a>Scrapy教程03- Spider详解</h1><p>Spider是爬虫框架的核心，爬取流程如下：</p>
<ol>
<li>先初始化请求URL列表，并指定下载后处理response的回调函数。初次请求URL通过<code>start_urls</code>指定，调用<code>start_requests()</code>产生<code>Request</code>对象，然后注册<code>parse</code>方法作为回调</li>
<li>在parse回调中解析response并返回字典,<code>Item</code>对象,<code>Request</code>对象或它们的迭代对象。<code>Request</code>对象还会包含回调函数，之后Scrapy下载完后会被这里注册的回调函数处理。</li>
<li>在回调函数里面，你通过使用选择器（同样可以使用BeautifulSoup,lxml或其他工具）解析页面内容，并生成解析后的结果Item。</li>
<li>最后返回的这些Item通常会被持久化到数据库中(使用<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">Item Pipeline</a>)或者使用<a href="http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports" target="_blank" rel="noopener">Feed exports</a>将其保存到文件中。</li>
</ol>
<p>尽管这个流程适合于所有的蜘蛛，但是Scrapy里面为不同的使用目的实现了一些常见的Spider。下面我们把它们列出来。</p>
<h2 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h2><p>链接爬取蜘蛛，专门为那些爬取有特定规律的链接内容而准备的。<br>如果你觉得它还不足以适合你的需求，可以先继承它然后覆盖相应的方法，或者自定义Spider也行。</p>
<p>它除了从<code>scrapy.Spider</code>类继承的属性外，还有一个新的属性<code>rules</code>,它是一个<code>Rule</code>对象列表，每个<code>Rule</code>对象定义了某个规则，如果多个<code>Rule</code>匹配一个连接，那么使用第一个，根据定义的顺序。</p>
<p>一个详细的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"link"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 提取匹配正则式'/group?f=index_group'链接 (但是不能匹配'deny.php')</span></span><br><span class="line">        <span class="comment"># 并且会递归爬取(如果没有定义callback，默认follow=True).</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'/group?f=index_group'</span>, ), deny=(<span class="string">'deny\.php'</span>, ))),</span><br><span class="line">        <span class="comment"># 提取匹配'/article/\d+/\d+.html'的链接，并使用parse_item来解析它们下载后的内容，不递归</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'/article/\d+/\d+\.html'</span>, )), callback=<span class="string">'parse_item'</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Hi, this is an item page! %s'</span>, response.url)</span><br><span class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</span><br><span class="line">        item = HuxiuItem()</span><br><span class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'link'</span>] = response.url</span><br><span class="line">        item[<span class="string">'posttime'</span>] = detail.xpath(</span><br><span class="line">            <span class="string">'div[@class="article-author"]/span[@class="article-time"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'posttime'</span>])</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p>
<h2 id="XMLFeedSpider"><a href="#XMLFeedSpider" class="headerlink" title="XMLFeedSpider"></a>XMLFeedSpider</h2><p>XML订阅蜘蛛，用来爬取XML形式的订阅内容，通过某个指定的节点来遍历。<br>可使用<code>iternodes</code>, <code>xml</code>, 和<code>html</code>三种形式的迭代器，不过当内容比较多的时候推荐使用<code>iternodes</code>，<br>默认也是它，可以节省内存提升性能，不需要将整个DOM加载到内存中再解析。而使用<code>html</code>可以处理XML有格式错误的内容。<br>处理XML的时候最好先<a href="http://doc.scrapy.org/en/1.0/topics/selectors.html#removing-namespaces" target="_blank" rel="noopener">Removing namespaces</a></p>
<p>接下来我通过爬取我的博客订阅XML来展示它的使用方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> BlogItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> XMLFeedSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XMLSpider</span><span class="params">(XMLFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"xml"</span></span><br><span class="line">    namespaces = [(<span class="string">'atom'</span>, <span class="string">'http://www.w3.org/2005/Atom'</span>)]</span><br><span class="line">    allowed_domains = [<span class="string">"github.io"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.pycoding.com/atom.xml"</span></span><br><span class="line">    ]</span><br><span class="line">    iterator = <span class="string">'xml'</span>  <span class="comment"># 缺省的iternodes，貌似对于有namespace的xml不行</span></span><br><span class="line">    itertag = <span class="string">'atom:entry'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_node</span><span class="params">(self, response, node)</span>:</span></span><br><span class="line">        <span class="comment"># self.logger.info('Hi, this is a &lt;%s&gt; node!', self.itertag)</span></span><br><span class="line">        item = BlogItem()</span><br><span class="line">        item[<span class="string">'title'</span>] = node.xpath(<span class="string">'atom:title/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'link'</span>] = node.xpath(<span class="string">'atom:link/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'id'</span>] = node.xpath(<span class="string">'atom:id/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'published'</span>] = node.xpath(<span class="string">'atom:published/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'updated'</span>] = node.xpath(<span class="string">'atom:updated/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        self.logger.info(<span class="string">'|'</span>.join([item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'id'</span>],item[<span class="string">'published'</span>]]))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h2 id="CSVFeedSpider"><a href="#CSVFeedSpider" class="headerlink" title="CSVFeedSpider"></a>CSVFeedSpider</h2><p>这个跟上面的XMLFeedSpider很类似，区别在于它会一行一行的迭代，而不是一个节点一个节点的迭代。<br>每次迭代行的时候会调用<code>parse_row()</code>方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> BlogItem</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CSVFeedSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CSVSpider</span><span class="params">(CSVFeedSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"csv"</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/feed.csv'</span>]</span><br><span class="line">    delimiter = <span class="string">';'</span></span><br><span class="line">    quotechar = <span class="string">"'"</span></span><br><span class="line">    headers = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'description'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_row</span><span class="params">(self, response, row)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Hi, this is a row!: %r'</span>, row)</span><br><span class="line">        item = BlogItem()</span><br><span class="line">        item[<span class="string">'id'</span>] = row[<span class="string">'id'</span>]</span><br><span class="line">        item[<span class="string">'name'</span>] = row[<span class="string">'name'</span>]</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h2 id="SitemapSpider"><a href="#SitemapSpider" class="headerlink" title="SitemapSpider"></a>SitemapSpider</h2><p>站点地图蜘蛛，允许你使用<a href="http://www.sitemaps.org/" target="_blank" rel="noopener">Sitemaps</a>发现URL后爬取整个站点。<br>还支持嵌套的站点地图以及从<code>robots.txt</code>中发现站点URL</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-Spider详解/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-Selector详解/">scrapy-Selector详解</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程04-Selector详解"><a href="#Scrapy教程04-Selector详解" class="headerlink" title="Scrapy教程04- Selector详解"></a>Scrapy教程04- Selector详解</h1><p>在你爬取网页的时候，最普遍的事情就是在页面源码中提取需要的数据，我们有几个库可以帮你完成这个任务：</p>
<ol>
<li><a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">BeautifulSoup</a>是python中一个非常流行的抓取库,<br>它还能合理的处理错误格式的标签，但是有一个唯一缺点就是：它运行很慢。</li>
<li><a href="http://lxml.de/" target="_blank" rel="noopener">lxml</a>是一个基于<a href="https://docs.python.org/2/library/xml.etree.elementtree.html" target="_blank" rel="noopener">ElementTree</a>的XML解析库(同时还能解析HTML),<br>不过lxml并不是Python标准库</li>
</ol>
<p>而Scrapy实现了自己的数据提取机制，它们被称为选择器，通过<a href="http://www.w3.org/TR/xpath" target="_blank" rel="noopener">XPath</a>或<a href="http://www.w3.org/TR/selectors" target="_blank" rel="noopener">CSS</a>表达式在HTML文档中来选择特定的部分</p>
<p><a href="http://www.w3.org/TR/xpath" target="_blank" rel="noopener">XPath</a>是一用来在XML中选择节点的语言，同时可以用在HTML上面。<br><a href="http://www.w3.org/TR/selectors" target="_blank" rel="noopener">CSS</a>是一种HTML文档上面的样式语言。</p>
<p>Scrapy选择器构建在lxml基础之上，所以可以保证速度和准确性。</p>
<p>本章我们来详细讲解下选择器的工作原理，还有它们极其简单和相似的API，比lxml的API少多了，因为lxml可以用于很多其他领域。</p>
<p>完整的API请查看<a href="http://doc.scrapy.org/en/latest/topics/selectors.html#topics-selectors-ref" target="_blank" rel="noopener">Selector参考</a></p>
<h2 id="关于选择器"><a href="#关于选择器" class="headerlink" title="关于选择器"></a>关于选择器</h2><p>Scrapy帮我们下载完页面后，我们怎样在满是html标签的内容中找到我们所需要的元素呢，这里就需要使用到选择器了，它们是用来定位元素并且提取元素的值。先来举几个例子看看：</p>
<ul>
<li>/html/head/title: 选择<code>&lt;title&gt;</code>节点, 它位于html文档的<code>&lt;head&gt;</code>节点内</li>
<li>/html/head/title/text(): 选择上面的<code>&lt;title&gt;</code>节点的内容.</li>
<li>//td: 选择页面中所有的<td>元素</td></li>
<li>//div[@class=”mine”]: 选择所有拥有属性<code>class=&quot;mine&quot;</code>的div元素</li>
</ul>
<p>Scrapy使用css和xpath选择器来定位元素，它有四个基本方法：</p>
<ul>
<li>xpath(): 返回选择器列表，每个选择器代表使用xpath语法选择的节点</li>
<li>css(): 返回选择器列表，每个选择器代表使用css语法选择的节点</li>
<li>extract(): 返回被选择元素的unicode字符串</li>
<li>re(): 返回通过正则表达式提取的unicode字符串列表</li>
</ul>
<h2 id="使用选择器"><a href="#使用选择器" class="headerlink" title="使用选择器"></a>使用选择器</h2><p>下面我们通过Scrapy shell演示下选择器的使用，假设我们有如下的一个网页<a href="http://doc.scrapy.org/en/latest/_static/selectors-sample1.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a>，内容如下：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">'http://example.com/'</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">'images'</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image1.html'</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image1_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image2.html'</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image2_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image3.html'</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image3_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image4.html'</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image4_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">'image5.html'</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'image5_thumb.jpg'</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>首先我们打开shell<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</span><br></pre></td></tr></table></figure></p>
<p>运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&apos;//title/text()&apos;)</span><br><span class="line">[&lt;Selector (text) xpath=//title/text()&gt;]</span><br><span class="line">&gt;&gt;&gt; response.css(&apos;title::text&apos;)</span><br><span class="line">[&lt;Selector (text) xpath=//title/text()&gt;]</span><br></pre></td></tr></table></figure></p>
<p>结果可以看出,<code>xpath()</code>和<code>css()</code>方法返回的是<code>SelectorList</code>实例，是一个选择器列表，你可以选择嵌套的数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&apos;img&apos;).xpath(&apos;@src&apos;).extract()</span><br><span class="line">[u&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image2_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image3_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image4_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure></p>
<p>必须使用<code>.extract()</code>才能提取最终的数据，如果你只想获得第一个匹配的，可以使用<code>.extract_first()</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&apos;//div[@id=&quot;images&quot;]/a/text()&apos;).extract_first()</span><br><span class="line">u&apos;Name: My image 1 &apos;</span><br></pre></td></tr></table></figure></p>
<p>如果没有找到，会返回<code>None</code>，还可选择默认值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&apos;//div[@id=&quot;not-exists&quot;]/text()&apos;).extract_first(default=&apos;not-found&apos;)</span><br><span class="line">&apos;not-found&apos;</span><br></pre></td></tr></table></figure></p>
<p>而CSS选择器还可以使用CSS3标准：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&apos;title::text&apos;).extract()</span><br><span class="line">[u&apos;Example website&apos;]</span><br></pre></td></tr></table></figure></p>
<p>下面是几个比较全面的示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&apos;//base/@href&apos;).extract()</span><br><span class="line">[u&apos;http://example.com/&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(&apos;base::attr(href)&apos;).extract()</span><br><span class="line">[u&apos;http://example.com/&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/@href&apos;).extract()</span><br><span class="line">[u&apos;image1.html&apos;,</span><br><span class="line"> u&apos;image2.html&apos;,</span><br><span class="line"> u&apos;image3.html&apos;,</span><br><span class="line"> u&apos;image4.html&apos;,</span><br><span class="line"> u&apos;image5.html&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(&apos;a[href*=image]::attr(href)&apos;).extract()</span><br><span class="line">[u&apos;image1.html&apos;,</span><br><span class="line"> u&apos;image2.html&apos;,</span><br><span class="line"> u&apos;image3.html&apos;,</span><br><span class="line"> u&apos;image4.html&apos;,</span><br><span class="line"> u&apos;image5.html&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/img/@src&apos;).extract()</span><br><span class="line">[u&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image2_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image3_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image4_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image5_thumb.jpg&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(&apos;a[href*=image] img::attr(src)&apos;).extract()</span><br><span class="line">[u&apos;image1_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image2_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image3_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image4_thumb.jpg&apos;,</span><br><span class="line"> u&apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure></p>
<h2 id="嵌套选择器"><a href="#嵌套选择器" class="headerlink" title="嵌套选择器"></a>嵌套选择器</h2><p><code>xpath()</code>和<code>css()</code>返回的是选择器列表，所以你可以继续使用它们的方法。举例来讲：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; links = response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]&apos;)</span><br><span class="line">&gt;&gt;&gt; links.extract()</span><br><span class="line">[u&apos;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</span><br><span class="line"> u&apos;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</span><br><span class="line"> u&apos;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</span><br><span class="line"> u&apos;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;,</span><br><span class="line"> u&apos;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; for index, link in enumerate(links):</span><br><span class="line">...     args = (index, link.xpath(&apos;@href&apos;).extract(), link.xpath(&apos;img/@src&apos;).extract())</span><br><span class="line">...     print &apos;Link number %d points to url %s and image %s&apos; % args</span><br><span class="line"></span><br><span class="line">Link number 0 points to url [u&apos;image1.html&apos;] and image [u&apos;image1_thumb.jpg&apos;]</span><br><span class="line">Link number 1 points to url [u&apos;image2.html&apos;] and image [u&apos;image2_thumb.jpg&apos;]</span><br><span class="line">Link number 2 points to url [u&apos;image3.html&apos;] and image [u&apos;image3_thumb.jpg&apos;]</span><br><span class="line">Link number 3 points to url [u&apos;image4.html&apos;] and image [u&apos;image4_thumb.jpg&apos;]</span><br><span class="line">Link number 4 points to url [u&apos;image5.html&apos;] and image [u&apos;image5_thumb.jpg&apos;]</span><br></pre></td></tr></table></figure></p>
<h2 id="使用正则表达式"><a href="#使用正则表达式" class="headerlink" title="使用正则表达式"></a>使用正则表达式</h2><p><code>Selector</code>有一个<code>re()</code>方法通过正则表达式提取数据，它返回的是unicode字符串列表，你不能再去嵌套使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/text()&apos;).re(r&apos;Name:\s*(.*)&apos;)</span><br><span class="line">[u&apos;My image 1&apos;,</span><br><span class="line"> u&apos;My image 2&apos;,</span><br><span class="line"> u&apos;My image 3&apos;,</span><br><span class="line"> u&apos;My image 4&apos;,</span><br><span class="line"> u&apos;My image 5&apos;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.xpath(&apos;//a[contains(@href, &quot;image&quot;)]/text()&apos;).re_first(r&apos;Name:\s*(.*)&apos;)</span><br><span class="line">u&apos;My image 1&apos;</span><br></pre></td></tr></table></figure></p>
<h2 id="XPath相对路径"><a href="#XPath相对路径" class="headerlink" title="XPath相对路径"></a>XPath相对路径</h2><p>当你嵌套使用XPath时候，不要使用<code>/</code>开头的，因为这个会相对文档根节点开始算起，需要使用相对路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; divs = response.xpath(&apos;//div&apos;)</span><br><span class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;.//p&apos;):  # extracts all &lt;p&gt; inside</span><br><span class="line">...     print p.extract()</span><br><span class="line"></span><br><span class="line"># 或者下面这个直接使用p也可以</span><br><span class="line">&gt;&gt;&gt; for p in divs.xpath(&apos;p&apos;):</span><br><span class="line">...     print p.extract()</span><br></pre></td></tr></table></figure></p>
<h2 id="XPath建议"><a href="#XPath建议" class="headerlink" title="XPath建议"></a>XPath建议</h2><h3 id="使用text作为条件时"><a href="#使用text作为条件时" class="headerlink" title="使用text作为条件时"></a>使用text作为条件时</h3><p>避免使用<code>.//text()</code>,直接使用<code>.</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sel.xpath(&quot;//a[contains(., &apos;Next Page&apos;)]&quot;).extract()</span><br><span class="line">[u&apos;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&apos;]</span><br></pre></td></tr></table></figure></p>
<h3 id="node-1-和-node-1-区别"><a href="#node-1-和-node-1-区别" class="headerlink" title="//node[1]和(//node)[1]区别"></a>//node[1]和(//node)[1]区别</h3><ul>
<li>//node[1]: 选择所有位于第一个子节点位置的node节点</li>
<li>(//node)[1]: 选择所有的node节点，然后返回结果中的第一个node节点</li>
</ul>
<h3 id="通过class查找时优先考虑CSS"><a href="#通过class查找时优先考虑CSS" class="headerlink" title="通过class查找时优先考虑CSS"></a>通过class查找时优先考虑CSS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; from scrapy import Selector</span><br><span class="line">&gt;&gt;&gt; sel = Selector(text=&apos;&lt;div class=&quot;hero shout&quot;&gt;&lt;time datetime=&quot;2014-07-23 19:00&quot;&gt;Special date&lt;/time&gt;&lt;/div&gt;&apos;)</span><br><span class="line">&gt;&gt;&gt; sel.css(&apos;.shout&apos;).xpath(&apos;./time/@datetime&apos;).extract()</span><br><span class="line">[u&apos;2014-07-23 19:00&apos;]</span><br></pre></td></tr></table></figure>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-Selector详解/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-Item详解/">scrapy-Item详解</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程05-Item详解"><a href="#Scrapy教程05-Item详解" class="headerlink" title="Scrapy教程05- Item详解"></a>Scrapy教程05- Item详解</h1><p>Item是保存结构数据的地方，Scrapy可以将解析结果以字典形式返回，但是Python中字典缺少结构，在大型爬虫系统中很不方便。</p>
<p>Item提供了类字典的API，并且可以很方便的声明字段，很多Scrapy组件可以利用Item的其他信息。</p>
<h2 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h2><p>定义Item非常简单，只需要继承<code>scrapy.Item</code>类，并将所有字段都定义为<code>scrapy.Field</code>类型即可<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    stock = scrapy.Field()</span><br><span class="line">    last_updated = scrapy.Field(serializer=str)</span><br></pre></td></tr></table></figure></p>
<h2 id="Item-Fields"><a href="#Item-Fields" class="headerlink" title="Item Fields"></a>Item Fields</h2><p><code>Field</code>对象可用来对每个字段指定元数据。例如上面<code>last_updated</code>的序列化函数指定为<code>str</code>，可任意指定元数据，不过每种元数据对于不同的组件意义不一样。</p>
<h2 id="Item使用示例"><a href="#Item使用示例" class="headerlink" title="Item使用示例"></a>Item使用示例</h2><p>你会看到Item的使用跟Python中的字典API非常类似</p>
<h3 id="创建Item"><a href="#创建Item" class="headerlink" title="创建Item"></a>创建Item</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product = Product(name=<span class="string">'Desktop PC'</span>, price=<span class="number">1000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> product</span><br><span class="line">Product(name=<span class="string">'Desktop PC'</span>, price=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<h3 id="获取值"><a href="#获取值" class="headerlink" title="获取值"></a>获取值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'name'</span>]</span><br><span class="line">Desktop PC</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'name'</span>)</span><br><span class="line">Desktop PC</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'price'</span>]</span><br><span class="line"><span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">KeyError: <span class="string">'last_updated'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'last_updated'</span>, <span class="string">'not set'</span>)</span><br><span class="line"><span class="keyword">not</span> set</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>] <span class="comment"># getting unknown field</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">KeyError: <span class="string">'lala'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product.get(<span class="string">'lala'</span>, <span class="string">'unknown field'</span>)</span><br><span class="line"><span class="string">'unknown field'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'name'</span> <span class="keyword">in</span> product  <span class="comment"># is name field populated?</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product  <span class="comment"># is last_updated populated?</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'last_updated'</span> <span class="keyword">in</span> product.fields  <span class="comment"># is last_updated a declared field?</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'lala'</span> <span class="keyword">in</span> product.fields  <span class="comment"># is lala a declared field?</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="设置值"><a href="#设置值" class="headerlink" title="设置值"></a>设置值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>] = <span class="string">'today'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'last_updated'</span>]</span><br><span class="line">today</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product[<span class="string">'lala'</span>] = <span class="string">'test'</span> <span class="comment"># setting unknown field</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">KeyError: <span class="string">'Product does not support field: lala'</span></span><br></pre></td></tr></table></figure>
<h3 id="访问所有的值"><a href="#访问所有的值" class="headerlink" title="访问所有的值"></a>访问所有的值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>product.keys()</span><br><span class="line">[<span class="string">'price'</span>, <span class="string">'name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>product.items()</span><br><span class="line">[(<span class="string">'price'</span>, <span class="number">1000</span>), (<span class="string">'name'</span>, <span class="string">'Desktop PC'</span>)]</span><br></pre></td></tr></table></figure>
<h2 id="Item-Loader"><a href="#Item-Loader" class="headerlink" title="Item Loader"></a>Item Loader</h2><p>Item Loader为我们提供了生成Item的相当便利的方法。Item为抓取的数据提供了容器，而Item Loader可以让我们非常方便的将输入填充到容器中。</p>
<p>下面我们通过一个例子来展示一般使用方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    l = ItemLoader(item=Product(), response=response)</span><br><span class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_name"]'</span>)</span><br><span class="line">    l.add_xpath(<span class="string">'name'</span>, <span class="string">'//div[@class="product_title"]'</span>)</span><br><span class="line">    l.add_xpath(<span class="string">'price'</span>, <span class="string">'//p[@id="price"]'</span>)</span><br><span class="line">    l.add_css(<span class="string">'stock'</span>, <span class="string">'p#stock]'</span>)</span><br><span class="line">    l.add_value(<span class="string">'last_updated'</span>, <span class="string">'today'</span>) <span class="comment"># you can also use literal values</span></span><br><span class="line">    <span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure></p>
<p>注意上面的<code>name</code>字段是从两个xpath路径添累加后得到。</p>
<h2 id="输入-输出处理器"><a href="#输入-输出处理器" class="headerlink" title="输入/输出处理器"></a>输入/输出处理器</h2><p>每个Item Loader对每个<code>Field</code>都有一个输入处理器和一个输出处理器。输入处理器在数据被接受到时执行，当数据收集完后调用<code>ItemLoader.load_item()</code>时再执行输出处理器，返回最终结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">l = ItemLoader(Product(), some_selector)</span><br><span class="line">l.add_xpath(<span class="string">'name'</span>, xpath1) <span class="comment"># (1)</span></span><br><span class="line">l.add_xpath(<span class="string">'name'</span>, xpath2) <span class="comment"># (2)</span></span><br><span class="line">l.add_css(<span class="string">'name'</span>, css) <span class="comment"># (3)</span></span><br><span class="line">l.add_value(<span class="string">'name'</span>, <span class="string">'test'</span>) <span class="comment"># (4)</span></span><br><span class="line"><span class="keyword">return</span> l.load_item() <span class="comment"># (5)</span></span><br></pre></td></tr></table></figure></p>
<p>执行流程是这样的：</p>
<ol>
<li><code>xpath1</code>中的数据被提取出来，然后传输到<code>name</code>字段的输入处理器中，在输入处理器处理完后生成结果放在Item Loader里面(这时候没有赋值给item)</li>
<li><code>xpath2</code>数据被提取出来，然后传输给(1)中同样的输入处理器，因为它们都是<code>name</code>字段的处理器，然后处理结果被附加到(1)的结果后面</li>
<li>跟2一样</li>
<li>跟3一样，不过这次是直接的字面字符串值，先转换成一个单元素的可迭代对象再传给输入处理器</li>
<li>上面4步的数据被传输给<code>name</code>的输出处理器，将最终的结果赋值给<code>name</code>字段</li>
</ol>
<h2 id="自定义Item-Loader"><a href="#自定义Item-Loader" class="headerlink" title="自定义Item Loader"></a>自定义Item Loader</h2><p>使用类定义语法，下面是一个例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, MapCompose, Join</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line"></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line">    name_in = MapCompose(unicode.title)</span><br><span class="line">    name_out = Join()</span><br><span class="line"></span><br><span class="line">    price_in = MapCompose(unicode.strip)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ...</span></span><br></pre></td></tr></table></figure></p>
<p>通过<code>_in</code>和<code>_out</code>后缀来定义输入和输出处理器，并且还可以定义默认的<code>ItemLoader.default_input_processor</code>和<code>ItemLoader.default_input_processor</code>.</p>
<h2 id="在Field定义中声明输入-输出处理器"><a href="#在Field定义中声明输入-输出处理器" class="headerlink" title="在Field定义中声明输入/输出处理器"></a>在Field定义中声明输入/输出处理器</h2><p>还有个地方可以非常方便的添加输入/输出处理器，那就是直接在Field定义中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> Join, MapCompose, TakeFirst</span><br><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_price</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> value.isdigit():</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags),</span><br><span class="line">        output_processor=Join(),</span><br><span class="line">    )</span><br><span class="line">    price = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags, filter_price),</span><br><span class="line">        output_processor=TakeFirst(),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p>
<p>优先级：</p>
<ol>
<li>在Item Loader中定义的<code>field_in</code>和<code>field_out</code></li>
<li>Filed元数据(<code>input_processor</code>和<code>output_processor</code>关键字)</li>
<li>Item Loader中的默认的</li>
</ol>
<p>Tips：一般来讲，将输入处理器定义在Item Loader的定义中<code>field_in</code>，然后将输出处理器定义在Field元数据中</p>
<h2 id="Item-Loader上下文"><a href="#Item-Loader上下文" class="headerlink" title="Item Loader上下文"></a>Item Loader上下文</h2><p>Item Loader上下文被所有输入/输出处理器共享，比如你有一个解析长度的函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_length</span><span class="params">(text, loader_context)</span>:</span></span><br><span class="line">    unit = loader_context.get(<span class="string">'unit'</span>, <span class="string">'m'</span>)</span><br><span class="line">    <span class="comment"># ... length parsing code goes here ...</span></span><br><span class="line">    <span class="keyword">return</span> parsed_length</span><br></pre></td></tr></table></figure></p>
<p>初始化和修改上下文的值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loader = ItemLoader(product)</span><br><span class="line">loader.context[<span class="string">'unit'</span>] = <span class="string">'cm'</span></span><br><span class="line"></span><br><span class="line">loader = ItemLoader(product, unit=<span class="string">'cm'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    length_out = MapCompose(parse_length, unit=<span class="string">'cm'</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="内置的处理器"><a href="#内置的处理器" class="headerlink" title="内置的处理器"></a>内置的处理器</h2><ol>
<li><code>Identity</code> 啥也不做</li>
<li><code>TakeFirst</code> 返回第一个非空值，通常用作输出处理器</li>
<li><code>Join</code> 将结果连起来，默认使用空格’ ‘</li>
<li><code>Compose</code> 将函数链接起来形成管道流，产生最后的输出</li>
<li><code>MapCompose</code> 跟上面的<code>Compose</code>类似，区别在于内部结果在函数中的传递方式.<br>它的输入值是可迭代的，首先将第一个函数依次作用于所有值，产生新的可迭代输入，作为第二个函数的输入，最后生成的结果连起来返回最终值，一般用在输入处理器中。</li>
<li><code>SelectJmes</code> 使用json路径来查询值并返回结果</li>
</ol>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-Item详解/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-Item Pipeline/">scrapy-Item Pipeline</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程06-Item-Pipeline"><a href="#Scrapy教程06-Item-Pipeline" class="headerlink" title="Scrapy教程06- Item Pipeline"></a>Scrapy教程06- Item Pipeline</h1><p>当一个item被蜘蛛爬取到之后会被发送给Item Pipeline，然后多个组件按照顺序处理这个item。<br>每个Item Pipeline组件其实就是一个实现了一个简单方法的Python类。他们接受一个item并在上面执行逻辑，还能决定这个item到底是否还要继续往下传输，如果不要了就直接丢弃。</p>
<p>使用Item Pipeline的常用场景：</p>
<ul>
<li>清理HTML数据</li>
<li>验证被抓取的数据(检查item是否包含某些字段)</li>
<li>重复性检查(然后丢弃)</li>
<li>将抓取的数据存储到数据库中</li>
</ul>
<h2 id="编写自己的Pipeline"><a href="#编写自己的Pipeline" class="headerlink" title="编写自己的Pipeline"></a>编写自己的Pipeline</h2><p>定义一个Python类，然后实现方法<code>process_item(self, item, spider)</code>即可，返回一个字典或Item，或者抛出<code>DropItem</code>异常丢弃这个Item。</p>
<p>或者还可以实现下面几个方法：</p>
<ul>
<li><code>open_spider(self, spider)</code> 蜘蛛打开的时执行</li>
<li><code>close_spider(self, spider)</code> 蜘蛛关闭时执行</li>
<li><code>from_crawler(cls, crawler)</code> 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中</li>
</ul>
<h2 id="Item-Pipeline示例"><a href="#Item-Pipeline示例" class="headerlink" title="Item Pipeline示例"></a>Item Pipeline示例</h2><h3 id="价格验证"><a href="#价格验证" class="headerlink" title="价格验证"></a>价格验证</h3><p>我们通过一个价格验证例子来看看怎样使用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    vat_factor = <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'price'</span>]:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="string">'price_excludes_vat'</span>]:</span><br><span class="line">                item[<span class="string">'price'</span>] = item[<span class="string">'price'</span>] * self.vat_factor</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Missing price in %s"</span> % item)</span><br></pre></td></tr></table></figure></p>
<h3 id="将item写入json文件"><a href="#将item写入json文件" class="headerlink" title="将item写入json文件"></a>将item写入json文件</h3><p>下面的这个Pipeline将所有的item写入到一个单独的json文件，一行一个item<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'items.jl'</span>, <span class="string">'wb'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        line = json.dumps(dict(item)) + <span class="string">"\n"</span></span><br><span class="line">        self.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h3 id="将item存储到MongoDB中"><a href="#将item存储到MongoDB中" class="headerlink" title="将item存储到MongoDB中"></a>将item存储到MongoDB中</h3><p>这个例子使用<a href="http://api.mongodb.org/python/current/" target="_blank" rel="noopener">pymongo</a>来演示怎样讲item保存到MongoDB中。<br>MongoDB的地址和数据库名在配置中指定，这个例子主要是向你展示怎样使用<code>from_crawler()</code>方法，以及如何清理资源。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">'scrapy_items'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>, <span class="string">'items'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h3 id="重复过滤器"><a href="#重复过滤器" class="headerlink" title="重复过滤器"></a>重复过滤器</h3><p>假设我们的item里面的id字典是唯一的，但是我们的蜘蛛返回了多个相同id的item<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'id'</span>] <span class="keyword">in</span> self.ids_seen:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Duplicate item found: %s"</span> % item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.ids_seen.add(item[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h2 id="激活一个Item-Pipeline组件"><a href="#激活一个Item-Pipeline组件" class="headerlink" title="激活一个Item Pipeline组件"></a>激活一个Item Pipeline组件</h2><p>你必须在配置文件中将你需要激活的Pipline组件添加到<code>ITEM_PIPELINES</code>中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'myproject.pipelines.PricePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>后面的数字表示它的执行顺序，从低到高执行，范围0-1000</p>
<h2 id="Feed-exports"><a href="#Feed-exports" class="headerlink" title="Feed exports"></a>Feed exports</h2><p>这里顺便提下Feed exports，一般有的爬虫直接将爬取结果序列化到文件中，并保存到某个存储介质中。只需要在settings里面设置几个即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* FEED_FORMAT= json # json|jsonlines|csv|xml|pickle|marshal</span><br><span class="line">* FEED_URI= file:///tmp/export.csv|ftp://user:pass@ftp.example.com/path/to/export.csv|s3://aws_key:aws_secret@mybucket/path/to/export.csv|stdout:</span><br><span class="line">* FEED_EXPORT_FIELDS = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;] # 这个在导出csv的时候有用</span><br></pre></td></tr></table></figure></p>
<h2 id="请求和响应"><a href="#请求和响应" class="headerlink" title="请求和响应"></a>请求和响应</h2><p>Scrapy使用<code>Request</code>和<code>Response</code>对象来爬取网站。<code>Request</code>对象被蜘蛛生成，然后被传递给下载器，之后下载器处理这个<code>Request</code>后返回<code>Response</code>对象，然后返回给生成<code>Request</code>的这个蜘蛛。</p>
<h3 id="给回调函数传递额外的参数"><a href="#给回调函数传递额外的参数" class="headerlink" title="给回调函数传递额外的参数"></a>给回调函数传递额外的参数</h3><p><code>Request</code>对象生成的时候会通过关键字参数<code>callback</code>指定回调函数，<code>Response</code>对象被当做第一个参数传入，有时候我们想传递额外的参数，比如我们构建某个Item的时候，需要两步，第一步是链接属性，第二步是详情属性，可以指定<code>Request.meta</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page1</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    item = MyItem()</span><br><span class="line">    item[<span class="string">'main_url'</span>] = response.url</span><br><span class="line">    request = scrapy.Request(<span class="string">"http://www.example.com/some_page.html"</span>,</span><br><span class="line">                             callback=self.parse_page2)</span><br><span class="line">    request.meta[<span class="string">'item'</span>] = item</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page2</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    item = response.meta[<span class="string">'item'</span>]</span><br><span class="line">    item[<span class="string">'other_url'</span>] = response.url</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
<h3 id="Request子类"><a href="#Request子类" class="headerlink" title="Request子类"></a>Request子类</h3><p>Scrapy为各种不同的场景内置了很多Request子类，你还可以继承它自定义自己的请求类。</p>
<p><code>FormRequest</code>这个专门为form表单设计，模拟表单提交的示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> [FormRequest(url=<span class="string">"http://www.example.com/post/action"</span>,</span><br><span class="line">                    formdata=&#123;<span class="string">'name'</span>: <span class="string">'John Doe'</span>, <span class="string">'age'</span>: <span class="string">'27'</span>&#125;,</span><br><span class="line">                    callback=self.after_post)]</span><br></pre></td></tr></table></figure></p>
<p>我们再来一个例子模拟用户登录，使用了<code>FormRequest.from_response()</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    start_urls = [<span class="string">'http://www.example.com/users/login.php'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> scrapy.FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata=&#123;<span class="string">'username'</span>: <span class="string">'john'</span>, <span class="string">'password'</span>: <span class="string">'secret'</span>&#125;,</span><br><span class="line">            callback=self.after_login</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># check login succeed before going on</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"authentication failed"</span> <span class="keyword">in</span> response.body:</span><br><span class="line">            self.logger.error(<span class="string">"Login failed"</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># continue scraping with authenticated session...</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Response子类"><a href="#Response子类" class="headerlink" title="Response子类"></a>Response子类</h3><p>一个<code>scrapy.http.Response</code>对象代表了一个HTTP相应，通常是被下载器下载后得到，并交给Spider做进一步的处理。Response也有很多默认的子类，用于表示各种不同的响应类型。</p>
<ul>
<li>TextResponse 在基本<code>Response</code>类基础之上增加了编码功能，专门用于二进制数据比如图片、声音或其他媒体文件</li>
<li>HtmlResponse 此类是<code>TextResponse</code>的子类，通过查询HTML的<code>meta http-equiv</code>属性实现了编码自动发现</li>
<li>XmlResponse  此类是<code>TextResponse</code>的子类，通过查询XML声明实现编码自动发现</li>
</ul>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-Item Pipeline/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-抓取动态网站/">scrapy-抓取动态网站</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程12-抓取动态网站"><a href="#Scrapy教程12-抓取动态网站" class="headerlink" title="Scrapy教程12- 抓取动态网站"></a>Scrapy教程12- 抓取动态网站</h1><p>前面我们介绍的都是去抓取静态的网站页面，也就是说我们打开某个链接，它的内容全部呈现出来。<br>但是如今的互联网大部分的web页面都是动态的，经常逛的网站例如京东、淘宝等，商品列表都是js，并有Ajax渲染，<br>下载某个链接得到的页面里面含有异步加载的内容，这样再使用之前的方式我们根本获取不到异步加载的这些网页内容。</p>
<p>使用Javascript渲染和处理网页是种非常常见的做法，如何处理一个大量使用Javascript的页面是Scrapy爬虫开发中一个常见的问题，<br>这篇文章将说明如何在Scrapy爬虫中使用<a href="https://github.com/scrapy-plugins/scrapy-splash" target="_blank" rel="noopener">scrapy-splash</a>来处理页面中得Javascript。</p>
<h3 id="scrapy-splash简介"><a href="#scrapy-splash简介" class="headerlink" title="scrapy-splash简介"></a>scrapy-splash简介</h3><p>scrapy-splash利用<a href="https://github.com/scrapy/scrapy" target="_blank" rel="noopener">Splash</a>将javascript和Scrapy集成起来，使得Scrapy可以抓取动态网页。</p>
<p>Splash是一个javascript渲染服务，是实现了HTTP API的轻量级浏览器，底层基于Twisted和QT框架，Python语言编写。所以首先你得安装Splash实例</p>
<h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><p>官网建议使用docker容器安装方式Splash。那么首先你得先安装docker</p>
<p>参考<a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/" target="_blank" rel="noopener">官方安装文档</a>，这里我选择Ubuntu 12.04 LTS版本安装</p>
<p>升级内核版本，docker需要3.13内核<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install linux-image-generic-lts-trusty</span><br><span class="line">$ sudo reboot</span><br></pre></td></tr></table></figure></p>
<p>安装<code>CA</code>认证<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install apt-transport-https ca-certificates</span><br></pre></td></tr></table></figure></p>
<p>增加新的<code>GPG</code>key<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D</span><br></pre></td></tr></table></figure></p>
<p>打开<code>/etc/apt/sources.list.d/docker.list</code>，如果没有就创建一个，然后删除任何已存在的内容，再增加下面一句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb https://apt.dockerproject.org/repo ubuntu-precise main</span><br></pre></td></tr></table></figure></p>
<p>更新APT<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get purge lxc-docker</span><br><span class="line">$ apt-cache policy docker-engine</span><br></pre></td></tr></table></figure></p>
<p>安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install docker-engine</span><br></pre></td></tr></table></figure></p>
<p>启动docker服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service docker start</span><br></pre></td></tr></table></figure></p>
<p>验证是否启动成功<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure></p>
<p>上面这条命令会下载一个测试镜像并在容器中运行它，它会打印一个消息，然后退出。</p>
<h3 id="安装Splash"><a href="#安装Splash" class="headerlink" title="安装Splash"></a>安装Splash</h3><p>拉取镜像下来<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker pull scrapinghub/splash</span><br></pre></td></tr></table></figure></p>
<p>启动容器<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splash</span><br></pre></td></tr></table></figure></p>
<p>现在可以通过0.0.0.0:8050(http),8051(https),5023 (telnet)来访问Splash了。</p>
<h3 id="安装scrapy-splash"><a href="#安装scrapy-splash" class="headerlink" title="安装scrapy-splash"></a>安装scrapy-splash</h3><p>使用pip安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install scrapy-splash</span><br></pre></td></tr></table></figure></p>
<h3 id="配置scrapy-splash"><a href="#配置scrapy-splash" class="headerlink" title="配置scrapy-splash"></a>配置scrapy-splash</h3><p>在你的scrapy工程的配置文件<code>settings.py</code>中添加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPLASH_URL = <span class="string">'http://192.168.203.92:8050'</span></span><br></pre></td></tr></table></figure></p>
<p>添加Splash中间件，还是在<code>settings.py</code>中通过<code>DOWNLOADER_MIDDLEWARES</code>指定，并且修改<code>HttpCompressionMiddleware</code>的优先级<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">    <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>默认情况下，HttpProxyMiddleware的优先级是750，要把它放在Splash中间件后面</p>
<p>设置Splash自己的去重过滤器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br></pre></td></tr></table></figure></p>
<p>如果你使用Splash的Http缓存，那么还要指定一个自定义的缓存后台存储介质，scrapy-splash提供了一个<code>scrapy.contrib.httpcache.FilesystemCacheStorage</code>的子类<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTTPCACHE_STORAGE = <span class="string">'scrapy_splash.SplashAwareFSCacheStorage'</span></span><br></pre></td></tr></table></figure></p>
<p>如果你要使用其他的缓存存储，那么需要继承这个类并且将所有的<code>scrapy.util.request.request_fingerprint</code>调用替换成<code>scrapy_splash.splash_request_fingerprint</code></p>
<h3 id="使用scrapy-splash"><a href="#使用scrapy-splash" class="headerlink" title="使用scrapy-splash"></a>使用scrapy-splash</h3><h4 id="SplashRequest"><a href="#SplashRequest" class="headerlink" title="SplashRequest"></a>SplashRequest</h4><p>最简单的渲染请求的方式是使用<code>scrapy_splash.SplashRequest</code>，通常你应该选择使用这个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> SplashRequest(url, self.parse_result,</span><br><span class="line">    args=&#123;</span><br><span class="line">        <span class="comment"># optional; parameters passed to Splash HTTP API</span></span><br><span class="line">        <span class="string">'wait'</span>: <span class="number">0.5</span>,</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 'url' is prefilled from request url</span></span><br><span class="line">        <span class="comment"># 'http_method' is set to 'POST' for POST requests</span></span><br><span class="line">        <span class="comment"># 'body' is set to request body for POST requests</span></span><br><span class="line">    &#125;,</span><br><span class="line">    endpoint=<span class="string">'render.json'</span>, <span class="comment"># optional; default is render.html</span></span><br><span class="line">    splash_url=<span class="string">'&lt;url&gt;'</span>,     <span class="comment"># optional; overrides SPLASH_URL</span></span><br><span class="line">    slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  <span class="comment"># optional</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>另外，你还可以在普通的scrapy请求中传递<code>splash</code>请求meta关键字达到同样的效果<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(url, self.parse_result, meta=&#123;</span><br><span class="line">    <span class="string">'splash'</span>: &#123;</span><br><span class="line">        <span class="string">'args'</span>: &#123;</span><br><span class="line">            <span class="comment"># set rendering arguments here</span></span><br><span class="line">            <span class="string">'html'</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">'png'</span>: <span class="number">1</span>,</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 'url' is prefilled from request url</span></span><br><span class="line">            <span class="comment"># 'http_method' is set to 'POST' for POST requests</span></span><br><span class="line">            <span class="comment"># 'body' is set to request body for POST requests</span></span><br><span class="line">        &#125;,</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optional parameters</span></span><br><span class="line">        <span class="string">'endpoint'</span>: <span class="string">'render.json'</span>,  <span class="comment"># optional; default is render.json</span></span><br><span class="line">        <span class="string">'splash_url'</span>: <span class="string">'&lt;url&gt;'</span>,      <span class="comment"># optional; overrides SPLASH_URL</span></span><br><span class="line">        <span class="string">'slot_policy'</span>: scrapy_splash.SlotPolicy.PER_DOMAIN,</span><br><span class="line">        <span class="string">'splash_headers'</span>: &#123;&#125;,       <span class="comment"># optional; a dict with headers sent to Splash</span></span><br><span class="line">        <span class="string">'dont_process_response'</span>: <span class="literal">True</span>, <span class="comment"># optional, default is False</span></span><br><span class="line">        <span class="string">'dont_send_headers'</span>: <span class="literal">True</span>,  <span class="comment"># optional, default is False</span></span><br><span class="line">        <span class="string">'magic_response'</span>: <span class="literal">False</span>,    <span class="comment"># optional, default is True</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p>
<p>Splash API说明，使用<code>SplashRequest</code>是一个非常便利的工具来填充<code>request.meta[&#39;splash&#39;]</code>里的数据</p>
<ul>
<li>meta[‘splash’][‘args’] 包含了发往Splash的参数。</li>
<li>meta[‘splash’][‘endpoint’] 指定了Splash所使用的endpoint，默认是<a href="http://splash.readthedocs.org/en/latest/api.html#render-html" target="_blank" rel="noopener">render.html</a></li>
<li>meta[‘splash’][‘splash_url’] 覆盖了<code>settings.py</code>文件中配置的Splash URL</li>
<li>meta[‘splash’][‘splash_headers’] 运行你增加或修改发往Splash服务器的HTTP头部信息，注意这个不是修改发往远程web站点的HTTP头部</li>
<li>meta[‘splash’][‘dont_send_headers’] 如果你不想传递headers给Splash，将它设置成True</li>
<li>meta[‘splash’][‘slot_policy’] 让你自定义Splash请求的同步设置</li>
<li>meta[‘splash’][‘dont_process_response’] 当你设置成True后，<code>SplashMiddleware</code>不会修改默认的<code>scrapy.Response</code>请求。默认是会返回<code>SplashResponse</code>子类响应比如<code>SplashTextResponse</code></li>
<li>meta[‘splash’][‘magic_response’] 默认为True，Splash会自动设置Response的一些属性，比如<code>response.headers</code>,<code>response.body</code>等</li>
</ul>
<p>如果你想通过Splash来提交Form请求，可以使用<code>scrapy_splash.SplashFormRequest</code>，它跟<code>SplashRequest</code>使用是一样的。</p>
<h4 id="Responses"><a href="#Responses" class="headerlink" title="Responses"></a>Responses</h4><p>对于不同的Splash请求，scrapy-splash返回不同的Response子类</p>
<ul>
<li>SplashResponse 二进制响应，比如对/render.png的响应</li>
<li>SplashTextResponse 文本响应，比如对/render.html的响应</li>
<li>SplashJsonResponse JSON响应，比如对/render.json或使用Lua脚本的/execute的响应</li>
</ul>
<p>如果你只想使用标准的Response对象，就设置<code>meta[&#39;splash&#39;][&#39;dont_process_response&#39;]=True</code></p>
<p>所有这些Response会把<code>response.url</code>设置成原始请求URL(也就是你要渲染的页面URL)，而不是Splash endpoint的URL地址。实际地址通过<code>response.real_url</code>得到</p>
<h4 id="Session的处理"><a href="#Session的处理" class="headerlink" title="Session的处理"></a>Session的处理</h4><p>Splash本身是无状态的，那么为了支持scrapy-splash的session必须编写Lua脚本，使用<code>/execute</code><br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">main</span><span class="params">(splash)</span></span></span><br><span class="line">    splash:init_cookies(splash.args.cookies)</span><br><span class="line"></span><br><span class="line">    <span class="comment">-- ... your script</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        cookies = splash:get_cookies(),</span><br><span class="line">        <span class="comment">-- ... other results, e.g. html</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>而标准的scrapy session参数可以使用<code>SplashRequest</code>将cookie添加到当前Splash cookiejar中</p>
<h3 id="使用实例"><a href="#使用实例" class="headerlink" title="使用实例"></a>使用实例</h3><p>接下来我通过一个实际的例子来演示怎样使用，我选择爬取<a href="http://www.jd.com/" target="_blank" rel="noopener">京东网</a>首页的异步加载内容。</p>
<p>京东网打开首页的时候只会将导航菜单加载出来，其他具体首页内容都是异步加载的，下面有个”猜你喜欢”这个内容也是异步加载的，<br>我现在就通过爬取这个”猜你喜欢”这四个字来说明下普通的Scrapy爬取和通过使用了Splash加载异步内容的区别。</p>
<p>首先我们写个简单的测试Spider，不使用splash：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"test"</span></span><br><span class="line">    allowed_domains = [<span class="string">"jd.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.jd.com/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        logging.info(<span class="string">u'---------我这个是简单的直接获取京东网首页测试---------'</span>)</span><br><span class="line">        guessyou = response.xpath(<span class="string">'//div[@id="guessyou"]/div[1]/h2/text()'</span>).extract_first()</span><br><span class="line">        logging.info(<span class="string">u"find：%s"</span> % guessyou)</span><br><span class="line">        logging.info(<span class="string">u'---------------success----------------'</span>)</span><br></pre></td></tr></table></figure></p>
<p>然后运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2016-04-18 14:42:44 test_spider.py[line:20] INFO ---------我这个是简单的直接获取京东网首页测试---------</span><br><span class="line">2016-04-18 14:42:44 test_spider.py[line:22] INFO find：None</span><br><span class="line">2016-04-18 14:42:44 test_spider.py[line:23] INFO ---------------success----------------</span><br></pre></td></tr></table></figure></p>
<p>我找不到那个”猜你喜欢”这四个字</p>
<p>接下来我使用splash来爬取<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy_splash <span class="keyword">import</span> SplashRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"jd"</span></span><br><span class="line">    allowed_domains = [<span class="string">"jd.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.jd.com/"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        splash_args = &#123;</span><br><span class="line">            <span class="string">'wait'</span>: <span class="number">0.5</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(url, self.parse_result, endpoint=<span class="string">'render.html'</span>,</span><br><span class="line">                                args=splash_args)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_result</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        logging.info(<span class="string">u'----------使用splash爬取京东网首页异步加载内容-----------'</span>)</span><br><span class="line">        guessyou = response.xpath(<span class="string">'//div[@id="guessyou"]/div[1]/h2/text()'</span>).extract_first()</span><br><span class="line">        logging.info(<span class="string">u"find：%s"</span> % guessyou)</span><br><span class="line">        logging.info(<span class="string">u'---------------success----------------'</span>)</span><br></pre></td></tr></table></figure></p>
<p>运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2016-04-18 14:42:51 js_spider.py[line:36] INFO ----------使用splash爬取京东网首页异步加载内容-----------</span><br><span class="line">2016-04-18 14:42:51 js_spider.py[line:38] INFO find：猜你喜欢</span><br><span class="line">2016-04-18 14:42:51 js_spider.py[line:39] INFO ---------------success----------------</span><br></pre></td></tr></table></figure></p>
<p>可以看出结果里面已经找到了这个”猜你喜欢”，说明异步加载内容爬取成功！</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-抓取动态网站/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2019 ～ 2020 By bb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>