<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="This is myblog!"><meta name="keywords" content><meta name="author" content="bb,shuiyue75381@gmail.com"><meta name="copyright" content="bb"><title>【bb的博客】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!--link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!--link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!--script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"X7RY6NGU4G","apiKey":"bf7ed65264918bbc03f12b4cc1212d85","indexName":"myblog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">bb</div><div class="author-info-description">This is myblog!</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/bbkali" target="_blank">GitHub<i class="icon-dot bg-color3"></i></a><a class="links-button button-hover" href="mailto:shuiyue75381@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color9"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">78</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">64</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">30</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">bb的博客</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-文件与图片/">scrapy-文件与图片</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程08-文件与图片"><a href="#Scrapy教程08-文件与图片" class="headerlink" title="Scrapy教程08- 文件与图片"></a>Scrapy教程08- 文件与图片</h1><p>Scrapy为我们提供了可重用的<a href="http://doc.scrapy.org/en/1.0/topics/item-pipeline.html" target="_blank" rel="noopener">item pipelines</a>为某个特定的Item去下载文件。<br>通常来说你会选择使用Files Pipeline或Images Pipeline。</p>
<p>这两个管道都实现了：</p>
<ul>
<li>避免重复下载</li>
<li>可以指定下载后保存的地方(文件系统目录中,Amazon S3中)</li>
</ul>
<p>Images Pipeline为处理图片提供了额外的功能：</p>
<ul>
<li>将所有下载的图片格式转换成普通的JPG并使用RGB颜色模式</li>
<li>生成缩略图</li>
<li>检查图片的宽度和高度确保它们满足最小的尺寸限制</li>
</ul>
<p>管道同时会在内部保存一个被调度下载的URL列表，然后将包含相同媒体的相应关联到这个队列上来，从而防止了多个item共享这个媒体时重复下载。</p>
<h2 id="使用Files-Pipeline"><a href="#使用Files-Pipeline" class="headerlink" title="使用Files Pipeline"></a>使用Files Pipeline</h2><p>一般我们会按照下面的步骤来使用文件管道：</p>
<ol>
<li>在某个Spider中，你爬取一个item后，将相应的文件URL放入<code>file_urls</code>字段中</li>
<li>item被返回之后就会转交给item pipeline</li>
<li>当这个item到达<code>FilesPipeline</code>时，在<code>file_urls</code>字段中的URL列表会通过标准的Scrapy调度器和下载器来调度下载，并且优先级很高，在抓取其他页面前就被处理。而这个<code>item</code>会一直在这个pipeline中被锁定，直到所有的文件下载完成。</li>
<li>当文件被下载完之后，结果会被赋值给另一个<code>files</code>字段。这个字段包含一个关于下载文件新的字典列表，比如下载路径，源地址，文件校验码。<code>files</code>里面的顺序和<code>file_url</code>顺序是一致的。要是某个写文件下载出错就不会出现在这个<code>files</code>中了。</li>
</ol>
<h2 id="使用Images-Pipeline"><a href="#使用Images-Pipeline" class="headerlink" title="使用Images Pipeline"></a>使用Images Pipeline</h2><p><code>ImagesPipeline</code>跟<code>FilesPipeline</code>的使用差不多，不过使用的字段名不一样，<code>image_urls</code>保存图片URL地址，<code>images</code>保存下载后的图片信息。</p>
<p>使用<code>ImagesPipeline</code>的好处是你可以通过配置来提供额外的功能，比如生成文件缩略图，通过图片大小过滤需要下载的图片等。</p>
<p><code>ImagesPipeline</code>使用<a href="https://github.com/python-pillow/Pillow" target="_blank" rel="noopener">Pillow</a>来生成缩略图以及转换成标准的JPEG/RGB格式。因此你需要安装这个包，我们建议你使用Pillow而不是PIL。</p>
<h2 id="使用例子"><a href="#使用例子" class="headerlink" title="使用例子"></a>使用例子</h2><p>要使用媒体管道，请先在配置文件中打开它<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同时使用图片和文件管道</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">                  <span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>: <span class="number">1</span>,</span><br><span class="line">                  <span class="string">'scrapy.pipelines.files.FilesPipeline'</span>: <span class="number">2</span>,</span><br><span class="line">                 &#125;</span><br><span class="line">FILES_STORE = <span class="string">'/path/to/valid/dir'</span>  <span class="comment"># 文件存储路径</span></span><br><span class="line">IMAGES_STORE = <span class="string">'/path/to/valid/dir'</span> <span class="comment"># 图片存储路径</span></span><br><span class="line"><span class="comment"># 90 days of delay for files expiration</span></span><br><span class="line">FILES_EXPIRES = <span class="number">90</span></span><br><span class="line"><span class="comment"># 30 days of delay for images expiration</span></span><br><span class="line">IMAGES_EXPIRES = <span class="number">30</span></span><br><span class="line"><span class="comment"># 图片缩略图</span></span><br><span class="line">IMAGES_THUMBS = &#123;</span><br><span class="line">    <span class="string">'small'</span>: (<span class="number">50</span>, <span class="number">50</span>),</span><br><span class="line">    <span class="string">'big'</span>: (<span class="number">270</span>, <span class="number">270</span>),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 图片过滤器，最小高度和宽度</span></span><br><span class="line">IMAGES_MIN_HEIGHT = <span class="number">110</span></span><br><span class="line">IMAGES_MIN_WIDTH = <span class="number">110</span></span><br></pre></td></tr></table></figure></p>
<p>一个使用了缩略图的下载例子会生成如下图片：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br><span class="line">&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br><span class="line">&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</span><br></pre></td></tr></table></figure></p>
<p>然后，某个Item返回时，有<code>file_urls</code>或<code>image_urls</code>，并且存在相应的<code>files</code>或<code>images</code>字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ... other item fields ...</span></span><br><span class="line">    image_urls = scrapy.Field()</span><br><span class="line">    images = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h2 id="自定义媒体管道"><a href="#自定义媒体管道" class="headerlink" title="自定义媒体管道"></a>自定义媒体管道</h2><p>如果你还需要更加复杂的功能，想自定义下载媒体逻辑，请参考<a href="http://doc.scrapy.org/en/1.0/topics/media-pipeline.html#topics-media-pipeline-override" target="_blank" rel="noopener">扩展媒体管道</a></p>
<p>不管是扩展<code>FilesPipeline</code>还是<code>ImagesPipeline</code>,都只需重写下面两个方法</p>
<ul>
<li><code>get_media_requests(self, item, info)</code>,返回一个<code>Request</code>对象</li>
<li><code>item_completed(self, results, item, info)</code>,当上门的Request下载完成后回调这个方法，然后填充<code>files</code>或<code>images</code>字段</li>
</ul>
<p>下面是一个扩展<code>ImagesPipeline</code>的例子，我只取path信息，并将它赋给<code>image_paths</code>字段，而不是默认的<code>images</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyImagesPipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span><span class="params">(self, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">'image_urls'</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(image_url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        image_paths = [x[<span class="string">'path'</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">"Item contains no images"</span>)</span><br><span class="line">        item[<span class="string">'image_paths'</span>] = image_paths</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-文件与图片/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-完整示例/">scrapy-完整示例</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程02-完整示例"><a href="#Scrapy教程02-完整示例" class="headerlink" title="Scrapy教程02- 完整示例"></a>Scrapy教程02- 完整示例</h1><p>这篇文章我们通过一个比较完整的例子来教你使用Scrapy，我选择爬取<a href="http://www.huxiu.com/" target="_blank" rel="noopener">虎嗅网首页</a>的新闻列表。</p>
<p>这里我们将完成如下几个步骤：</p>
<ul>
<li>创建一个新的Scrapy工程</li>
<li>定义你所需要要抽取的Item对象</li>
<li>编写一个spider来爬取某个网站并提取出所有的Item对象</li>
<li>编写一个Item Pipline来存储提取出来的Item对象</li>
</ul>
<p>Scrapy使用Python语言编写，如果你对这门语言还不熟，请先去学习下基本知识。</p>
<h2 id="创建Scrapy工程"><a href="#创建Scrapy工程" class="headerlink" title="创建Scrapy工程"></a>创建Scrapy工程</h2><p>在任何你喜欢的目录执行如下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject coolscrapy</span><br></pre></td></tr></table></figure></p>
<p>将会创建coolscrapy文件夹，其目录结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">coolscrapy/</span><br><span class="line">    scrapy.cfg            # 部署配置文件</span><br><span class="line"></span><br><span class="line">    coolscrapy/           # Python模块，你所有的代码都放这里面</span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          # Item定义文件</span><br><span class="line"></span><br><span class="line">        pipelines.py      # pipelines定义文件</span><br><span class="line"></span><br><span class="line">        settings.py       # 配置文件</span><br><span class="line"></span><br><span class="line">        spiders/          # 所有爬虫spider都放这个文件夹下面</span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></p>
<h2 id="定义我们的Item"><a href="#定义我们的Item" class="headerlink" title="定义我们的Item"></a>定义我们的Item</h2><p>我们通过创建一个scrapy.Item类，并定义它的类型为scrapy.Field的属性，<br>我们准备将虎嗅网新闻列表的名称、链接地址和摘要爬取下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()    <span class="comment"># 标题</span></span><br><span class="line">    link = scrapy.Field()     <span class="comment"># 链接</span></span><br><span class="line">    desc = scrapy.Field()     <span class="comment"># 简述</span></span><br><span class="line">    posttime = scrapy.Field() <span class="comment"># 发布时间</span></span><br></pre></td></tr></table></figure>
<p>也许你觉得定义这个Item有点麻烦，但是定义完之后你可以得到许多好处，这样你就可以使用Scrapy中其他有用的组件和帮助类。</p>
<h2 id="第一个Spider"><a href="#第一个Spider" class="headerlink" title="第一个Spider"></a>第一个Spider</h2><p>蜘蛛就是你定义的一些类，Scrapy使用它们来从一个domain（或domain组）爬取信息。<br>在蜘蛛类中定义了一个初始化的URL下载列表，以及怎样跟踪链接，如何解析页面内容来提取Item。</p>
<p>定义一个Spider，只需继承<code>scrapy.Spider</code>类并定于一些属性：</p>
<ul>
<li>name: Spider名称，必须是唯一的</li>
<li>start_urls: 初始化下载链接URL</li>
<li>parse(): 用来解析下载后的Response对象，该对象也是这个方法的唯一参数。<br>它负责解析返回页面数据并提取出相应的Item（返回Item对象），还有其他合法的链接URL（返回Request对象）。</li>
</ul>
<p>我们在coolscrapy/spiders文件夹下面新建<code>huxiu_spider.py</code>，内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Topic: sample</span></span><br><span class="line"><span class="string">Desc :</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'desc'</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h2><p>在根目录执行下面的命令，其中huxiu是你定义的spider名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>如果一切正常，应该可以打印出每一个新闻</p>
<h2 id="处理链接"><a href="#处理链接" class="headerlink" title="处理链接"></a>处理链接</h2><p>如果想继续跟踪每个新闻链接进去，看看它的详细内容的话，那么可以在parse()方法中返回一个Request对象，<br>然后注册一个回调函数来解析新闻详情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># print(item['title'],item['link'],item['desc'])</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_article)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</span><br><span class="line">        item = HuxiuItem()</span><br><span class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'link'</span>] = response.url</span><br><span class="line">        item[<span class="string">'posttime'</span>] = detail.xpath(</span><br><span class="line">            <span class="string">'div[@class="article-author"]/span[@class="article-time"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'posttime'</span>])</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>现在parse只提取感兴趣的链接，然后将链接内容解析交给另外的方法去处理了。<br>你可以基于这个构建更加复杂的爬虫程序了。</p>
<h2 id="导出抓取数据"><a href="#导出抓取数据" class="headerlink" title="导出抓取数据"></a>导出抓取数据</h2><p>最简单的保存抓取数据的方式是使用json格式的文件保存在本地，像下面这样运行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu -o items.json</span><br></pre></td></tr></table></figure></p>
<p>在演示的小系统里面这种方式足够了。不过如果你要构建复杂的爬虫系统，<br>最好自己编写<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">Item Pipeline</a>。</p>
<h2 id="保存数据到数据库"><a href="#保存数据到数据库" class="headerlink" title="保存数据到数据库"></a>保存数据到数据库</h2><p>上面我们介绍了可以将抓取的Item导出为json格式的文件，不过最常见的做法还是编写Pipeline将其存储到数据库中。我们在<code>coolscrapy/pipelines.py</code>定义<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> News, db_connect, create_news_table, Article</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleDataBasePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""保存文章到数据库"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        engine = db_connect()</span><br><span class="line">        create_news_table(engine)</span><br><span class="line">        self.Session = sessionmaker(bind=engine)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""This method is called when the spider is opened."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        a = Article(url=item[<span class="string">"url"</span>],</span><br><span class="line">                    title=item[<span class="string">"title"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    publish_time=item[<span class="string">"publish_time"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    body=item[<span class="string">"body"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    source_site=item[<span class="string">"source_site"</span>].encode(<span class="string">"utf-8"</span>))</span><br><span class="line">        <span class="keyword">with</span> session_scope(self.Session) <span class="keyword">as</span> session:</span><br><span class="line">            session.add(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>上面我使用了python中的SQLAlchemy来保存数据库，这个是一个非常优秀的ORM库，我写了篇关于它的<a href="http://yidao620c.github.io/2016/03/07/sqlalchemy.html" target="_blank" rel="noopener">入门教程</a>，可以参考下。</p>
<p>然后在<code>setting.py</code>中配置这个Pipeline，还有数据库链接等信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'coolscrapy.pipelines.ArticleDataBasePipeline'</span>: <span class="number">5</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># linux pip install MySQL-python</span></span><br><span class="line">DATABASE = &#123;<span class="string">'drivername'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'host'</span>: <span class="string">'192.168.203.95'</span>,</span><br><span class="line">            <span class="string">'port'</span>: <span class="string">'3306'</span>,</span><br><span class="line">            <span class="string">'username'</span>: <span class="string">'root'</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'database'</span>: <span class="string">'spider'</span>,</span><br><span class="line">            <span class="string">'query'</span>: &#123;<span class="string">'charset'</span>: <span class="string">'utf8'</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>再次运行爬虫<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>那么所有新闻的文章都存储到数据库中去了。</p>
<h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><p>本章只是带你领略了scrapy最基本的功能，还有很多高级特性没有讲到。接下来会通过多个例子向你展示scrapy的其他特性，然后再深入讲述每个特性。</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-完整示例/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-入门篇/">scrapy-入门篇</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程01-入门篇"><a href="#Scrapy教程01-入门篇" class="headerlink" title="Scrapy教程01- 入门篇"></a>Scrapy教程01- 入门篇</h1><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，<br>信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取(更确切来说,网络抓取)所设计的，<br>也可以应用在获取API所返回的数据(比如Web Services)或者通用的网络爬虫。</p>
<p>Scrapy也能帮你实现高阶的爬虫框架，比如爬取时的网站认证、内容的分析处理、重复抓取、分布式爬取等等很复杂的事。</p>
<h2 id="安装scrapy"><a href="#安装scrapy" class="headerlink" title="安装scrapy"></a>安装scrapy</h2><p>我的测试环境是centos6.5</p>
<p>升级python到最新版的2.7，下面的所有步骤都切换到root用户</p>
<p>由于scrapy目前只能运行在python2上，所以先更新centos上面的python到最新的<br><a href="https://www.python.org/downloads/release/python-2711/" target="_blank" rel="noopener">Python 2.7.11</a>，<br>具体方法请google下很多这样的教程。</p>
<p>先安装一些依赖软件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install python-devel</span><br><span class="line">yum install libffi-devel</span><br><span class="line">yum install openssl-devel</span><br></pre></td></tr></table></figure></p>
<p>然后安装pyopenssl库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyopenssl</span><br></pre></td></tr></table></figure></p>
<p>安装xlml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install python-lxml</span><br><span class="line">yum install libxml2-devel</span><br><span class="line">yum install libxslt-devel</span><br></pre></td></tr></table></figure></p>
<p>安装service-identity<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install service-identity</span><br></pre></td></tr></table></figure></p>
<p>安装twisted<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure></p>
<p>安装scrapy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy -U</span><br></pre></td></tr></table></figure></p>
<p>测试scrapy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy bench</span><br></pre></td></tr></table></figure></p>
<p>最终成功，太不容易了！</p>
<h2 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h2><p>创建一个python源文件，名为stackoverflow.py，内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StackOverflowSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'stackoverflow'</span></span><br><span class="line">    start_urls = [<span class="string">'http://stackoverflow.com/questions?sort=votes'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> href <span class="keyword">in</span> response.css(<span class="string">'.question-summary h3 a::attr(href)'</span>):</span><br><span class="line">            full_url = response.urljoin(href.extract())</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(full_url, callback=self.parse_question)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_question</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: response.css(<span class="string">'h1 a::text'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'votes'</span>: response.css(<span class="string">'.question .vote-count-post::text'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'body'</span>: response.css(<span class="string">'.question .post-text'</span>).extract()[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'tags'</span>: response.css(<span class="string">'.question .post-tag::text'</span>).extract(),</span><br><span class="line">            <span class="string">'link'</span>: response.url,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json</span><br></pre></td></tr></table></figure></p>
<p>结果类似下面：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[&#123;</span><br><span class="line">    &quot;body&quot;: &quot;... LONG HTML HERE ...&quot;,</span><br><span class="line">    &quot;link&quot;: &quot;http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array&quot;,</span><br><span class="line">    &quot;tags&quot;: [&quot;java&quot;, &quot;c++&quot;, &quot;performance&quot;, &quot;optimization&quot;],</span><br><span class="line">    &quot;title&quot;: &quot;Why is processing a sorted array faster than an unsorted array?&quot;,</span><br><span class="line">    &quot;votes&quot;: &quot;9924&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">    &quot;body&quot;: &quot;... LONG HTML HERE ...&quot;,</span><br><span class="line">    &quot;link&quot;: &quot;http://stackoverflow.com/questions/1260748/how-do-i-remove-a-git-submodule&quot;,</span><br><span class="line">    &quot;tags&quot;: [&quot;git&quot;, &quot;git-submodules&quot;],</span><br><span class="line">    &quot;title&quot;: &quot;How do I remove a Git submodule?&quot;,</span><br><span class="line">    &quot;votes&quot;: &quot;1764&quot;</span><br><span class="line">&#125;,</span><br><span class="line">...]</span><br></pre></td></tr></table></figure></p>
<p>当你运行<code>scrapy runspider somefile.py</code>这条语句的时候，Scrapy会去寻找源文件中定义的一个spider并且交给爬虫引擎来执行它。<br><code>start_urls</code>属性定义了开始的URL，爬虫会通过它来构建初始的请求，返回response后再调用默认的回调方法<code>parse</code>并传入这个response。<br>我们在<code>parse</code>回调方法中通过使用css选择器提取每个提问页面链接的href属性值，然后<code>yield</code>另外一个请求，<br>并注册<code>parse_question</code>回调方法，在这个请求完成后被执行。</p>
<p>处理流程图：</p>
<p><img src="https://raw.githubusercontent.com/bbkali/picbad/master/2020-1-19-17-30-31" alt="2020-1-19-17-30-31"></p>
<p>Scrapy的一个好处是所有请求都是被调度并异步处理，就算某个请求出错也不影响其他请求继续被处理。</p>
<p>我们的示例中将解析结果生成json格式，你还可以导出为其他格式（比如XML、CSV），或者是将其存储到FTP、Amazon S3上。<br>你还可以通过<a href="http://doc.scrapy.org/en/1.0/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">pipeline</a><br>将它们存储到数据库中去，这些数据保存的方式各种各样。</p>
<h2 id="Scrapy特性一览"><a href="#Scrapy特性一览" class="headerlink" title="Scrapy特性一览"></a>Scrapy特性一览</h2><p>你已经可以通过Scrapy从一个网站上面爬取数据并将其解析保存下来了，但是这只是Scrapy的皮毛。<br>Scrapy提供了更多的特性来让你爬取更加容易和高效。比如：</p>
<ol>
<li>内置支持扩展的CSS选择器和XPath表达式来从HTML/XML源码中选择并提取数据，还能使用正则表达式</li>
<li>提供交互式shell控制台试验CSS和XPath表达式，这个在调试你的蜘蛛程序时很有用</li>
<li>内置支持生成多种格式的订阅导出（JSON、CSV、XML）并将它们存储在多个位置（FTP、S3、本地文件系统）</li>
<li>健壮的编码支持和自动识别，用于处理外文、非标准和错误编码问题</li>
<li>可扩展，允许你使用<a href="http://doc.scrapy.org/en/1.0/topics/signals.html#topics-signals" target="_blank" rel="noopener">signals</a><br>和友好的API(middlewares, extensions, 和pipelines)来编写自定义插件功能。</li>
<li>大量的内置扩展和中间件供使用：<ul>
<li>cookies and session handling</li>
<li>HTTP features like compression, authentication, caching</li>
<li>user-agent spoofing</li>
<li>robots.txt</li>
<li>crawl depth restriction</li>
<li>and more</li>
</ul>
</li>
<li>还有其他好多好东东，比如可重复利用蜘蛛来爬取<a href="http://www.sitemaps.org/" target="_blank" rel="noopener">Sitemaps</a>和XML/CSV订阅，<br>一个跟爬取元素关联的媒体管道来<br><a href="http://doc.scrapy.org/en/1.0/topics/media-pipeline.html#topics-media-pipeline" target="_blank" rel="noopener">自动下载图片</a>，<br>一个缓存DNS解析器等等。</li>
</ol>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-入门篇/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-内置服务/">scrapy-内置服务</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程07-内置服务"><a href="#Scrapy教程07-内置服务" class="headerlink" title="Scrapy教程07- 内置服务"></a>Scrapy教程07- 内置服务</h1><p>Scrapy使用Python内置的的日志系统来记录事件日志。<br>日志配置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">LOG_ENABLED = true</span><br><span class="line">LOG_ENCODING = <span class="string">"utf-8"</span></span><br><span class="line">LOG_LEVEL = logging.INFO</span><br><span class="line">LOG_FILE = <span class="string">"log/spider.log"</span></span><br><span class="line">LOG_STDOUT = <span class="literal">True</span></span><br><span class="line">LOG_FORMAT = <span class="string">"%(asctime)s [%(name)s] %(levelname)s: %(message)s"</span></span><br><span class="line">LOG_DATEFORMAT = <span class="string">"%Y-%m-%d %H:%M:%S"</span></span><br></pre></td></tr></table></figure></p>
<p>使用也很简单<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">logger.warning(<span class="string">"This is a warning"</span>)</span><br></pre></td></tr></table></figure></p>
<p>如果在Spider里面使用，那就更简单了，因为logger就是它的一个实例变量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line">    start_urls = [<span class="string">'http://scrapinghub.com'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.logger.info(<span class="string">'Parse function called on %s'</span>, response.url)</span><br></pre></td></tr></table></figure></p>
<h2 id="发送email"><a href="#发送email" class="headerlink" title="发送email"></a>发送email</h2><p>Scrapy发送email基于<a href="http://twistedmatrix.com/documents/current/core/howto/defer-intro.html" target="_blank" rel="noopener">Twisted non-blocking IO</a>实现，只需几个简单配置即可。</p>
<p>初始化<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mailer = MailSender.from_settings(settings)</span><br></pre></td></tr></table></figure></p>
<p>发送不包含附件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mailer.send(to=[<span class="string">"someone@example.com"</span>], subject=<span class="string">"Some subject"</span>, body=<span class="string">"Some body"</span>, cc=[<span class="string">"another@example.com"</span>])</span><br></pre></td></tr></table></figure></p>
<p>配置<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MAIL_FROM = <span class="string">'scrapy@localhost'</span></span><br><span class="line">MAIL_HOST = <span class="string">'localhost'</span></span><br><span class="line">MAIL_PORT = <span class="number">25</span></span><br><span class="line">MAIL_USER = <span class="string">""</span></span><br><span class="line">MAIL_PASS = <span class="string">""</span></span><br><span class="line">MAIL_TLS = <span class="literal">False</span></span><br><span class="line">MAIL_SSL = <span class="literal">False</span></span><br></pre></td></tr></table></figure></p>
<h2 id="同一个进程运行多个Spider"><a href="#同一个进程运行多个Spider" class="headerlink" title="同一个进程运行多个Spider"></a>同一个进程运行多个Spider</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider1</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your first spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider2</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your second spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">process = CrawlerProcess()</span><br><span class="line">process.crawl(MySpider1)</span><br><span class="line">process.crawl(MySpider2)</span><br><span class="line">process.start() <span class="comment"># the script will block here until all crawling jobs are finished</span></span><br></pre></td></tr></table></figure>
<h2 id="分布式爬虫"><a href="#分布式爬虫" class="headerlink" title="分布式爬虫"></a>分布式爬虫</h2><p>Scrapy并没有提供内置的分布式抓取功能，不过有很多方法可以帮你实现。</p>
<p>如果你有很多个spider，最简单的方式就是启动多个<code>Scrapyd</code>实例，然后将spider分布到各个机器上面。</p>
<p>如果你想多个机器运行同一个spider，可以将url分片后交给每个机器上面的spider。比如你把URL分成3份<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://somedomain.com/urls-to-crawl/spider1/part1.list</span><br><span class="line">http://somedomain.com/urls-to-crawl/spider1/part2.list</span><br><span class="line">http://somedomain.com/urls-to-crawl/spider1/part3.list</span><br></pre></td></tr></table></figure></p>
<p>然后运行3个<code>Scrapyd</code>实例，分别启动它们，并传递part参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1</span><br><span class="line">curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2</span><br><span class="line">curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3</span><br></pre></td></tr></table></figure></p>
<h2 id="防止被封的策略"><a href="#防止被封的策略" class="headerlink" title="防止被封的策略"></a>防止被封的策略</h2><p>一些网站实现了一些策略来禁止爬虫来爬取它们的网页。有的比较简单，有的相当复杂，如果你需要详细了解可以咨询<a href="http://scrapy.org/support/" target="_blank" rel="noopener">商业支持</a></p>
<p>下面是对于这些网站的一些有用的建议：</p>
<ul>
<li>使用user agent池。也就是每次发送的时候随机从池中选择不一样的浏览器头信息，防止暴露爬虫身份</li>
<li>禁止Cookie，某些网站会通过Cookie识别用户身份，禁用后使得服务器无法识别爬虫轨迹</li>
<li>设置download_delay下载延迟，数字设置为5秒，越大越安全</li>
<li>如果有可能的话尽量使用<a href="http://www.googleguide.com/cached_pages.html" target="_blank" rel="noopener">Google cache</a>获取网页，而不是直接访问</li>
<li>使用一个轮转IP池，例如免费的<a href="https://www.torproject.org/" target="_blank" rel="noopener">Tor project</a>或者是付费的<a href="http://proxymesh.com/" target="_blank" rel="noopener">ProxyMesh</a></li>
<li>使用大型分布式下载器，这样就能完全避免被封了，只需要关注怎样解析页面就行。一个例子就是<a href="http://scrapinghub.com/crawlera" target="_blank" rel="noopener">Crawlera</a></li>
</ul>
<p>如果这些还是无法避免被禁，可以考虑<a href="http://scrapy.org/support/" target="_blank" rel="noopener">商业支持</a></p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-内置服务/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-模拟登录/">scrapy-模拟登录</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程11-模拟登录"><a href="#Scrapy教程11-模拟登录" class="headerlink" title="Scrapy教程11- 模拟登录"></a>Scrapy教程11- 模拟登录</h1><p>有时候爬取网站的时候需要登录，在Scrapy中可以通过模拟登录保存cookie后再去爬取相应的页面。这里我通过登录github然后爬取自己的issue列表来演示下整个原理。</p>
<p>要想实现登录就需要表单提交，先通过浏览器访问github的登录页面<a href="https://github.com/login" target="_blank" rel="noopener">https://github.com/login</a>，<br>然后使用浏览器调试工具来得到登录时需要提交什么东西:</p>
<p><img src="https://raw.githubusercontent.com/bbkali/picbad/master/2020-1-19-17-31-18" alt="2020-1-19-17-31-18"></p>
<p>我这里使用chrome浏览器的调试工具，F12打开后选择Network，并将Preserve log勾上。<br>我故意输入错误的用户名和密码，得到它提交的form表单参数还有POST提交的URL:</p>
<p><img src="https://raw.githubusercontent.com/bbkali/picbad/master/2020-1-19-17-31-30" alt="2020-1-19-17-31-30"></p>
<p>去查看html源码会发现表单里面有个隐藏的<code>authenticity_token</code>值，这个是需要先获取然后跟用户名和密码一起提交的:</p>
<p><img src="https://raw.githubusercontent.com/bbkali/picbad/master/2020-1-19-17-31-39" alt="2020-1-19-17-31-39"></p>
<h3 id="重写start-requests方法"><a href="#重写start-requests方法" class="headerlink" title="重写start_requests方法"></a>重写start_requests方法</h3><p>要使用cookie，第一步得打开它呀，默认scrapy使用<code>CookiesMiddleware</code>中间件，并且打开了。如果你之前禁止过，请设置如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COOKIES_ENABLES = <span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<p>我们先要打开登录页面，获取<code>authenticity_token</code>值，这里我重写了start_requests方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [Request(<span class="string">"https://github.com/login"</span>,</span><br><span class="line">                    meta=&#123;<span class="string">'cookiejar'</span>: <span class="number">1</span>&#125;, callback=self.post_login)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># FormRequeset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># 先去拿隐藏的表单参数authenticity_token</span></span><br><span class="line">    authenticity_token = response.xpath(</span><br><span class="line">        <span class="string">'//input[@name="authenticity_token"]/@value'</span>).extract_first()</span><br><span class="line">    logging.info(<span class="string">'authenticity_token='</span> + authenticity_token)</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p><code>start_requests</code>方法指定了回调函数，用来获取隐藏表单值<code>authenticity_token</code>，同时我们还给Request指定了<code>cookiejar</code>的元数据，用来往回调函数传递cookie标识。</p>
<h3 id="使用FormRequest"><a href="#使用FormRequest" class="headerlink" title="使用FormRequest"></a>使用FormRequest</h3><p>Scrapy为我们准备了<code>FormRequest</code>类专门用来进行Form表单提交的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了模拟浏览器，我们定义httpheader</span></span><br><span class="line">post_headers = &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"</span>,</span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">    <span class="string">"Accept-Language"</span>: <span class="string">"zh-CN,zh;q=0.8,en;q=0.6"</span>,</span><br><span class="line">    <span class="string">"Cache-Control"</span>: <span class="string">"no-cache"</span>,</span><br><span class="line">    <span class="string">"Connection"</span>: <span class="string">"keep-alive"</span>,</span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36"</span>,</span><br><span class="line">    <span class="string">"Referer"</span>: <span class="string">"https://github.com/"</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 使用FormRequeset模拟表单提交</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># 先去拿隐藏的表单参数authenticity_token</span></span><br><span class="line">    authenticity_token = response.xpath(</span><br><span class="line">        <span class="string">'//input[@name="authenticity_token"]/@value'</span>).extract_first()</span><br><span class="line">    logging.info(<span class="string">'authenticity_token='</span> + authenticity_token)</span><br><span class="line">    <span class="comment"># FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></span><br><span class="line">    <span class="comment"># 登陆成功后, 会调用after_login回调函数，如果url跟Request页面的一样就省略掉</span></span><br><span class="line">    <span class="keyword">return</span> [FormRequest.from_response(response,</span><br><span class="line">                                      url=<span class="string">'https://github.com/session'</span>,</span><br><span class="line">                                      meta=&#123;<span class="string">'cookiejar'</span>: response.meta[<span class="string">'cookiejar'</span>]&#125;,</span><br><span class="line">                                      headers=self.post_headers,  <span class="comment"># 注意此处的headers</span></span><br><span class="line">                                      formdata=&#123;</span><br><span class="line">                                          <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">                                          <span class="string">'login'</span>: <span class="string">'yidao620c'</span>,</span><br><span class="line">                                          <span class="string">'password'</span>: <span class="string">'******'</span>,</span><br><span class="line">                                          <span class="string">'authenticity_token'</span>: authenticity_token</span><br><span class="line">                                      &#125;,</span><br><span class="line">                                      callback=self.after_login,</span><br><span class="line">                                      dont_filter=<span class="literal">True</span></span><br><span class="line">                                      )]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p><code>FormRequest.from_response()</code>方法让你指定提交的url，请求头还有form表单值，注意我们还通过<code>meta</code>传递了cookie标识。它同样有个回调函数，登录成功后调用。下面我们来实现它<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># 登录之后，开始进入我要爬取的私信页面</span></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">        <span class="comment"># 因为我们上面定义了Rule，所以只需要简单的生成初始爬取Request即可</span></span><br><span class="line">        <span class="keyword">yield</span> Request(url, meta=&#123;<span class="string">'cookiejar'</span>: response.meta[<span class="string">'cookiejar'</span>]&#125;)</span><br></pre></td></tr></table></figure></p>
<p>这里我通过<code>start_urls</code>定义了开始页面，然后生成Request，具体爬取的规则和<code>下一页</code>规则在前面的Rule里定义了。注意这里我继续传递<code>cookiejar</code>，访问初始页面时带上cookie信息。</p>
<h3 id="重写-requests-to-follow"><a href="#重写-requests-to-follow" class="headerlink" title="重写_requests_to_follow"></a>重写_requests_to_follow</h3><p>有个问题刚开始困扰我很久就是这里我定义的spider继承自CrawlSpider，它内部自动去下载匹配的链接，而每次去访问链接的时候并没有自动带上cookie，后来我重写了它的<code>_requests_to_follow()</code>方法解决了这个问题<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""重写加入cookiejar的更新"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    seen = set()</span><br><span class="line">    <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">        links = [l <span class="keyword">for</span> l <span class="keyword">in</span> rule.link_extractor.extract_links(response) <span class="keyword">if</span> l <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">        <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">            links = rule.process_links(links)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            seen.add(link)</span><br><span class="line">            r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">            <span class="comment"># 下面这句是我重写的</span></span><br><span class="line">            r.meta.update(rule=n, link_text=link.text, cookiejar=response.meta[<span class="string">'cookiejar'</span>])</span><br><span class="line">            <span class="keyword">yield</span> rule.process_request(r)</span><br></pre></td></tr></table></figure></p>
<h3 id="页面处理方法"><a href="#页面处理方法" class="headerlink" title="页面处理方法"></a>页面处理方法</h3><p>在规则Rule里面我定义了每个链接的回调函数<code>parse_page</code>，就是最终我们处理每个issue页面提取信息的逻辑<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="string">"""这个是使用LinkExtractor自动处理链接以及`下一页`"""</span></span><br><span class="line">    logging.info(<span class="string">u'--------------消息分割线-----------------'</span>)</span><br><span class="line">    logging.info(response.url)</span><br><span class="line">    issue_title = response.xpath(</span><br><span class="line">        <span class="string">'//span[@class="js-issue-title"]/text()'</span>).extract_first()</span><br><span class="line">    logging.info(<span class="string">u'issue_title：'</span> + issue_title.encode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="完整源码"><a href="#完整源码" class="headerlink" title="完整源码"></a>完整源码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Topic: 登录爬虫</span></span><br><span class="line"><span class="string">Desc : 模拟登录https://github.com后将自己的issue全部爬出来</span></span><br><span class="line"><span class="string">tips：使用chrome调试post表单的时候勾选Preserve log和Disable cache</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request, FormRequest, HtmlResponse</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">                    format=<span class="string">'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s'</span>,</span><br><span class="line">                    datefmt=<span class="string">'%Y-%m-%d %H:%M:%S'</span>,</span><br><span class="line">                    handlers=[logging.StreamHandler(sys.stdout)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GithubSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"github"</span></span><br><span class="line">    allowed_domains = [<span class="string">"github.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'https://github.com/issues'</span>,</span><br><span class="line">    ]</span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># 消息列表</span></span><br><span class="line">        Rule(LinkExtractor(allow=(<span class="string">'/issues/\d+'</span>,),</span><br><span class="line">                           restrict_xpaths=<span class="string">'//ul[starts-with(@class, "table-list")]/li/div[2]/a[2]'</span>),</span><br><span class="line">             callback=<span class="string">'parse_page'</span>),</span><br><span class="line">        <span class="comment"># 下一页, If callback is None follow defaults to True, otherwise it defaults to False</span></span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths=<span class="string">'//a[@class="next_page"]'</span>)),</span><br><span class="line">    )</span><br><span class="line">    post_headers = &#123;</span><br><span class="line">        <span class="string">"Accept"</span>: <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"</span>,</span><br><span class="line">        <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>,</span><br><span class="line">        <span class="string">"Accept-Language"</span>: <span class="string">"zh-CN,zh;q=0.8,en;q=0.6"</span>,</span><br><span class="line">        <span class="string">"Cache-Control"</span>: <span class="string">"no-cache"</span>,</span><br><span class="line">        <span class="string">"Connection"</span>: <span class="string">"keep-alive"</span>,</span><br><span class="line">        <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>,</span><br><span class="line">        <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.75 Safari/537.36"</span>,</span><br><span class="line">        <span class="string">"Referer"</span>: <span class="string">"https://github.com/"</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [Request(<span class="string">"https://github.com/login"</span>,</span><br><span class="line">                        meta=&#123;<span class="string">'cookiejar'</span>: <span class="number">1</span>&#125;, callback=self.post_login)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FormRequeset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 先去拿隐藏的表单参数authenticity_token</span></span><br><span class="line">        authenticity_token = response.xpath(</span><br><span class="line">            <span class="string">'//input[@name="authenticity_token"]/@value'</span>).extract_first()</span><br><span class="line">        logging.info(<span class="string">'authenticity_token='</span> + authenticity_token)</span><br><span class="line">        <span class="comment"># FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单</span></span><br><span class="line">        <span class="comment"># 登陆成功后, 会调用after_login回调函数，如果url跟Request页面的一样就省略掉</span></span><br><span class="line">        <span class="keyword">return</span> [FormRequest.from_response(response,</span><br><span class="line">                                          url=<span class="string">'https://github.com/session'</span>,</span><br><span class="line">                                          meta=&#123;<span class="string">'cookiejar'</span>: response.meta[<span class="string">'cookiejar'</span>]&#125;,</span><br><span class="line">                                          headers=self.post_headers,  <span class="comment"># 注意此处的headers</span></span><br><span class="line">                                          formdata=&#123;</span><br><span class="line">                                              <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">                                              <span class="string">'login'</span>: <span class="string">'yidao620c'</span>,</span><br><span class="line">                                              <span class="string">'password'</span>: <span class="string">'******'</span>,</span><br><span class="line">                                              <span class="string">'authenticity_token'</span>: authenticity_token</span><br><span class="line">                                          &#125;,</span><br><span class="line">                                          callback=self.after_login,</span><br><span class="line">                                          dont_filter=<span class="literal">True</span></span><br><span class="line">                                          )]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">            <span class="comment"># 因为我们上面定义了Rule，所以只需要简单的生成初始爬取Request即可</span></span><br><span class="line">            <span class="keyword">yield</span> Request(url, meta=&#123;<span class="string">'cookiejar'</span>: response.meta[<span class="string">'cookiejar'</span>]&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""这个是使用LinkExtractor自动处理链接以及`下一页`"""</span></span><br><span class="line">        logging.info(<span class="string">u'--------------消息分割线-----------------'</span>)</span><br><span class="line">        logging.info(response.url)</span><br><span class="line">        issue_title = response.xpath(</span><br><span class="line">            <span class="string">'//span[@class="js-issue-title"]/text()'</span>).extract_first()</span><br><span class="line">        logging.info(<span class="string">u'issue_title：'</span> + issue_title.encode(<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="string">"""重写加入cookiejar的更新"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        seen = set()</span><br><span class="line">        <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">            links = [l <span class="keyword">for</span> l <span class="keyword">in</span> rule.link_extractor.extract_links(response) <span class="keyword">if</span> l <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">            <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">                links = rule.process_links(links)</span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">                seen.add(link)</span><br><span class="line">                r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">                <span class="comment"># 下面这句是我重写的</span></span><br><span class="line">                r.meta.update(rule=n, link_text=link.text, cookiejar=response.meta[<span class="string">'cookiejar'</span>])</span><br><span class="line">                <span class="keyword">yield</span> rule.process_request(r)</span><br></pre></td></tr></table></figure>
<p>你可以在<a href="https://github.com/yidao620c/core-scrapy" target="_blank" rel="noopener">GitHub</a>上看到本文的完整项目源码，还有另外一个自动登陆iteye网站的例子。</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-模拟登录/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/3/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2019 ～ 2020 By bb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>