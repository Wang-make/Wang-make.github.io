<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="This is myblog!"><meta name="keywords" content><meta name="author" content="bb,shuiyue75381@gmail.com"><meta name="copyright" content="bb"><title>【bb的博客】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!--link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!--link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!--script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"X7RY6NGU4G","apiKey":"bf7ed65264918bbc03f12b4cc1212d85","indexName":"myblog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="author-info"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">bb</div><div class="author-info-description">This is myblog!</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/bbkali" target="_blank">GitHub<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="mailto:shuiyue75381@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color2"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">78</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">64</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">30</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">bb的博客</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><div id="recent-posts"><!-- each post in page.posts.sort('date', -1).limit(10).toArray()--><!-- config中配置按照什么排序--><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-动态配置爬虫/">scrapy-动态配置爬虫</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程10-动态配置爬虫"><a href="#Scrapy教程10-动态配置爬虫" class="headerlink" title="Scrapy教程10- 动态配置爬虫"></a>Scrapy教程10- 动态配置爬虫</h1><p>有很多时候我们需要从多个网站爬取所需要的数据，比如我们想爬取多个网站的新闻，将其存储到数据库同一个表中。我们是不是要对每个网站都得去定义一个Spider类呢？<br>其实不需要，我们可以通过维护一个规则配置表或者一个规则配置文件来动态增加或修改爬取规则，然后程序代码不需要更改就能实现多个网站爬取。</p>
<p>要这样做，我们就不能再使用前面的<code>scrapy crawl test</code>这种命令了，我们需要使用编程的方式运行Scrapy spider，参考<a href="http://doc.scrapy.org/en/1.0/topics/practices.html#run-scrapy-from-a-script" target="_blank" rel="noopener">官方文档</a></p>
<h3 id="脚本运行Scrapy"><a href="#脚本运行Scrapy" class="headerlink" title="脚本运行Scrapy"></a>脚本运行Scrapy</h3><p>可以利用scrapy提供的<a href="http://doc.scrapy.org/en/1.0/topics/api.html#topics-api" target="_blank" rel="noopener">核心API</a>通过编程方式启动scrapy，代替传统的<code>scrapy crawl</code>启动方式。</p>
<p>Scrapy构建于Twisted异步网络框架基础之上，因此你需要在Twisted reactor里面运行。</p>
<p>首先你可以使用<code>scrapy.crawler.CrawlerProcess</code>这个类来运行你的spider，这个类会为你启动一个Twisted reactor，并能配置你的日志和shutdown处理器。所有的scrapy命令都使用这个类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line">process = CrawlerProcess(get_project_settings())</span><br><span class="line"></span><br><span class="line">process.crawl(MySpider)</span><br><span class="line">process.start() <span class="comment"># the script will block here until the crawling is finished</span></span><br></pre></td></tr></table></figure></p>
<p>然后你就可以直接执行这个脚本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run.py</span><br></pre></td></tr></table></figure></p>
<p>另外一个功能更强大的类是<code>scrapy.crawler.CrawlerRunner</code>，推荐你使用这个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">configure_logging(&#123;<span class="string">'LOG_FORMAT'</span>: <span class="string">'%(levelname)s: %(message)s'</span>&#125;)</span><br><span class="line">runner = CrawlerRunner()</span><br><span class="line"></span><br><span class="line">d = runner.crawl(MySpider)</span><br><span class="line">d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line">reactor.run() <span class="comment"># the script will block here until the crawling is finished</span></span><br></pre></td></tr></table></figure></p>
<h3 id="同一进程运行多个spider"><a href="#同一进程运行多个spider" class="headerlink" title="同一进程运行多个spider"></a>同一进程运行多个spider</h3><p>默认情况当你每次执行<code>scrapy crawl</code>命令时会创建一个新的进程。但我们可以使用核心API在同一个进程中同时运行多个spider<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider1</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your first spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider2</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your second spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">configure_logging()</span><br><span class="line">runner = CrawlerRunner()</span><br><span class="line">runner.crawl(MySpider1)</span><br><span class="line">runner.crawl(MySpider2)</span><br><span class="line">d = runner.join()</span><br><span class="line">d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line"></span><br><span class="line">reactor.run() <span class="comment"># the script will block here until all crawling jobs are finished</span></span><br></pre></td></tr></table></figure></p>
<h3 id="定义规则表"><a href="#定义规则表" class="headerlink" title="定义规则表"></a>定义规则表</h3><p>好了言归正传，有了前面的脚本启动基础，就可以开始我们的动态配置爬虫了。<br>我们的需求是这样的，从两个不同的网站爬取我们所需要的新闻文章，然后存储到article表中。</p>
<p>首先我们需要定义规则表和文章表，通过动态的创建蜘蛛类，我们以后就只需要维护规则表即可了。这里我使用SQLAlchemy框架来映射数据库。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Topic: 定义数据库模型实体</span></span><br><span class="line"><span class="string">Desc :</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sqlalchemy.engine.url <span class="keyword">import</span> URL</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.ext.declarative <span class="keyword">import</span> declarative_base</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine, Column, Integer, String, Text, DateTime</span><br><span class="line"><span class="keyword">from</span> coolscrapy.settings <span class="keyword">import</span> DATABASE</span><br><span class="line"></span><br><span class="line">Base = declarative_base()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleRule</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""自定义文章爬取规则"""</span></span><br><span class="line">    __tablename__ = <span class="string">'article_rule'</span></span><br><span class="line"></span><br><span class="line">    id = Column(Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 规则名称</span></span><br><span class="line">    name = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 运行的域名列表，逗号隔开</span></span><br><span class="line">    allow_domains = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 开始URL列表，逗号隔开</span></span><br><span class="line">    start_urls = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 下一页的xpath</span></span><br><span class="line">    next_page = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 文章链接正则表达式(子串)</span></span><br><span class="line">    allow_url = Column(String(<span class="number">200</span>))</span><br><span class="line">    <span class="comment"># 文章链接提取区域xpath</span></span><br><span class="line">    extract_from = Column(String(<span class="number">200</span>))</span><br><span class="line">    <span class="comment"># 文章标题xpath</span></span><br><span class="line">    title_xpath = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 文章内容xpath</span></span><br><span class="line">    body_xpath = Column(Text)</span><br><span class="line">    <span class="comment"># 发布时间xpath</span></span><br><span class="line">    publish_time_xpath = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 文章来源</span></span><br><span class="line">    source_site = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 规则是否生效</span></span><br><span class="line">    enable = Column(Integer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Article</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""文章类"""</span></span><br><span class="line">    __tablename__ = <span class="string">'articles'</span></span><br><span class="line"></span><br><span class="line">    id = Column(Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    url = Column(String(<span class="number">100</span>))</span><br><span class="line">    title = Column(String(<span class="number">100</span>))</span><br><span class="line">    body = Column(Text)</span><br><span class="line">    publish_time = Column(String(<span class="number">30</span>))</span><br><span class="line">    source_site = Column(String(<span class="number">30</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="定义文章Item"><a href="#定义文章Item" class="headerlink" title="定义文章Item"></a>定义文章Item</h3><p>这个很简单了，没什么需要说明的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Article</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    body = scrapy.Field()</span><br><span class="line">    publish_time = scrapy.Field()</span><br><span class="line">    source_site = scrapy.Field()</span><br></pre></td></tr></table></figure></p>
<h3 id="定义ArticleSpider"><a href="#定义ArticleSpider" class="headerlink" title="定义ArticleSpider"></a>定义ArticleSpider</h3><p>接下来我们将定义爬取文章的蜘蛛，这个spider会使用一个Rule实例来初始化，然后根据Rule实例中的xpath规则来获取相应的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.utils <span class="keyword">import</span> parse_text</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> Article</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"article"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rule)</span>:</span></span><br><span class="line">        self.rule = rule</span><br><span class="line">        self.name = rule.name</span><br><span class="line">        self.allowed_domains = rule.allow_domains.split(<span class="string">","</span>)</span><br><span class="line">        self.start_urls = rule.start_urls.split(<span class="string">","</span>)</span><br><span class="line">        rule_list = []</span><br><span class="line">        <span class="comment"># 添加`下一页`的规则</span></span><br><span class="line">        <span class="keyword">if</span> rule.next_page:</span><br><span class="line">            rule_list.append(Rule(LinkExtractor(restrict_xpaths=rule.next_page)))</span><br><span class="line">        <span class="comment"># 添加抽取文章链接的规则</span></span><br><span class="line">        rule_list.append(Rule(LinkExtractor(</span><br><span class="line">            allow=[rule.allow_url],</span><br><span class="line">            restrict_xpaths=[rule.extract_from]),</span><br><span class="line">            callback=<span class="string">'parse_item'</span>))</span><br><span class="line">        self.rules = tuple(rule_list)</span><br><span class="line">        super(ArticleSpider, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'Hi, this is an article page! %s'</span> % response.url)</span><br><span class="line"></span><br><span class="line">        article = Article()</span><br><span class="line">        article[<span class="string">"url"</span>] = response.url</span><br><span class="line"></span><br><span class="line">        title = response.xpath(self.rule.title_xpath).extract()</span><br><span class="line">        article[<span class="string">"title"</span>] = parse_text(title, self.rule.name, <span class="string">'title'</span>)</span><br><span class="line"></span><br><span class="line">        body = response.xpath(self.rule.body_xpath).extract()</span><br><span class="line">        article[<span class="string">"body"</span>] = parse_text(body, self.rule.name, <span class="string">'body'</span>)</span><br><span class="line"></span><br><span class="line">        publish_time = response.xpath(self.rule.publish_time_xpath).extract()</span><br><span class="line">        article[<span class="string">"publish_time"</span>] = parse_text(publish_time, self.rule.name, <span class="string">'publish_time'</span>)</span><br><span class="line"></span><br><span class="line">        article[<span class="string">"source_site"</span>] = self.rule.source_site</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> article</span><br></pre></td></tr></table></figure></p>
<p>要注意的是start_urls，rules等都初始化成了对象的属性，都由传入的rule对象初始化，parse_item方法中的抽取规则也都有rule对象提供。</p>
<h3 id="编写pipeline存储到数据库中"><a href="#编写pipeline存储到数据库中" class="headerlink" title="编写pipeline存储到数据库中"></a>编写pipeline存储到数据库中</h3><p>我们还是使用SQLAlchemy来将文章Item数据存储到数据库中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">session_scope</span><span class="params">(Session)</span>:</span></span><br><span class="line">    <span class="string">"""Provide a transactional scope around a series of operations."""</span></span><br><span class="line">    session = Session()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">yield</span> session</span><br><span class="line">        session.commit()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        session.rollback()</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        session.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleDataBasePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""保存文章到数据库"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        engine = db_connect()</span><br><span class="line">        create_news_table(engine)</span><br><span class="line">        self.Session = sessionmaker(bind=engine)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""This method is called when the spider is opened."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        a = Article(url=item[<span class="string">"url"</span>],</span><br><span class="line">                    title=item[<span class="string">"title"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    publish_time=item[<span class="string">"publish_time"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    body=item[<span class="string">"body"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    source_site=item[<span class="string">"source_site"</span>].encode(<span class="string">"utf-8"</span>))</span><br><span class="line">        <span class="keyword">with</span> session_scope(self.Session) <span class="keyword">as</span> session:</span><br><span class="line">            session.add(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<h3 id="修改run-py启动脚本"><a href="#修改run-py启动脚本" class="headerlink" title="修改run.py启动脚本"></a>修改run.py启动脚本</h3><p>我们将上面的run.py稍作修改即可定制我们的文章爬虫启动脚本<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> spiders.article_spider <span class="keyword">import</span> ArticleSpider</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> db_connect</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> ArticleRule</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    settings = get_project_settings()</span><br><span class="line">    configure_logging(settings)</span><br><span class="line">    db = db_connect()</span><br><span class="line">    Session = sessionmaker(bind=db)</span><br><span class="line">    session = Session()</span><br><span class="line">    rules = session.query(ArticleRule).filter(ArticleRule.enable == <span class="number">1</span>).all()</span><br><span class="line">    session.close()</span><br><span class="line">    runner = CrawlerRunner(settings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> rules:</span><br><span class="line">        <span class="comment"># stop reactor when spider closes</span></span><br><span class="line">        <span class="comment"># runner.signals.connect(spider_closing, signal=signals.spider_closed)</span></span><br><span class="line">        runner.crawl(ArticleSpider, rule=rule)</span><br><span class="line"></span><br><span class="line">    d = runner.join()</span><br><span class="line">    d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># blocks process so always keep as the last statement</span></span><br><span class="line">    reactor.run()</span><br><span class="line">    logging.info(<span class="string">'all finished.'</span>)</span><br></pre></td></tr></table></figure></p>
<p>OK，一切搞定。现在我们可以往ArticleRule表中加入成百上千个网站的规则，而不用添加一行代码，就可以对这成百上千个网站进行爬取。<br>当然你完全可以做一个Web前端来完成维护ArticleRule表的任务。当然ArticleRule规则也可以放在除了数据库的任何地方，比如配置文件。</p>
<p>你可以在<a href="https://github.com/yidao620c/core-scrapy" target="_blank" rel="noopener">GitHub</a>上看到本文的完整项目源码。</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-动态配置爬虫/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/19/scrapy-部署/">scrapy-部署</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div><div class="post-content"><div class="main-content content"><h1 id="Scrapy教程09-部署"><a href="#Scrapy教程09-部署" class="headerlink" title="Scrapy教程09- 部署"></a>Scrapy教程09- 部署</h1><p>本篇主要介绍两种部署爬虫的方案。如果仅仅在开发调试的时候在本地部署跑起来是很容易的，不过要是生产环境，爬虫任务量大，并且持续时间长，那么还是建议使用专业的部署方法。主要是两种方案：</p>
<ul>
<li><a href="http://doc.scrapy.org/en/1.0/topics/deploy.html#deploy-scrapyd" target="_blank" rel="noopener">Scrapyd</a> 开源方案</li>
<li><a href="http://doc.scrapy.org/en/1.0/topics/deploy.html#deploy-scrapy-cloud" target="_blank" rel="noopener">Scrapy Cloud</a> 云方案</li>
</ul>
<h2 id="部署到Scrapyd"><a href="#部署到Scrapyd" class="headerlink" title="部署到Scrapyd"></a>部署到Scrapyd</h2><p><a href="http://doc.scrapy.org/en/1.0/topics/deploy.html#deploy-scrapyd" target="_blank" rel="noopener">Scrapyd</a>是一个开源软件，用来运行蜘蛛爬虫。它提供了HTTP API的服务器，还能运行和监控Scrapy的蜘蛛</p>
<p>要部署爬虫到Scrapyd，需要使用到<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">scrapyd-client</a>部署工具集，下面我演示下部署的步骤</p>
<p>Scrapyd通常以守护进程daemon形式运行，监听spider的请求，然后为每个spider创建一个进程执行<code>scrapy crawl myspider</code>,同时Scrapyd还能以多进程方式启动，通过配置<code>max_proc</code>和<code>max_proc_per_cpu</code>选项</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>使用pip安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapyd</span><br></pre></td></tr></table></figure></p>
<p>在ubuntu系统上面<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install scrapyd</span><br></pre></td></tr></table></figure></p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置文件地址，优先级从低到高</p>
<ul>
<li>/etc/scrapyd/scrapyd.conf (Unix)</li>
<li>/etc/scrapyd/conf.d/* (in alphabetical order, Unix)</li>
<li>scrapyd.conf</li>
<li>~/.scrapyd.conf (users home directory)</li>
</ul>
<p>具体参数参考<a href="http://scrapyd.readthedocs.org/en/latest/config.html" target="_blank" rel="noopener">scrapyd配置</a></p>
<p>简单的例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[scrapyd]</span><br><span class="line">eggs_dir    = eggs</span><br><span class="line">logs_dir    = logs</span><br><span class="line">items_dir   =</span><br><span class="line">jobs_to_keep = 5</span><br><span class="line">dbs_dir     = dbs</span><br><span class="line">max_proc    = 0</span><br><span class="line">max_proc_per_cpu = 4</span><br><span class="line">finished_to_keep = 100</span><br><span class="line">poll_interval = 5</span><br><span class="line">bind_address = 0.0.0.0</span><br><span class="line">http_port   = 6800</span><br><span class="line">debug       = off</span><br><span class="line">runner      = scrapyd.runner</span><br><span class="line">application = scrapyd.app.application</span><br><span class="line">launcher    = scrapyd.launcher.Launcher</span><br><span class="line">webroot     = scrapyd.website.Root</span><br><span class="line"></span><br><span class="line">[services]</span><br><span class="line">schedule.json     = scrapyd.webservice.Schedule</span><br><span class="line">cancel.json       = scrapyd.webservice.Cancel</span><br><span class="line">addversion.json   = scrapyd.webservice.AddVersion</span><br><span class="line">listprojects.json = scrapyd.webservice.ListProjects</span><br><span class="line">listversions.json = scrapyd.webservice.ListVersions</span><br><span class="line">listspiders.json  = scrapyd.webservice.ListSpiders</span><br><span class="line">delproject.json   = scrapyd.webservice.DeleteProject</span><br><span class="line">delversion.json   = scrapyd.webservice.DeleteVersion</span><br><span class="line">listjobs.json     = scrapyd.webservice.ListJobs</span><br><span class="line">daemonstatus.json = scrapyd.webservice.DaemonStatus</span><br></pre></td></tr></table></figure></p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>使用<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">scrapyd-client</a>最方便，<br>Scrapyd-client是<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">scrapyd</a>的一个客户端，它提供了<code>scrapyd-deploy</code>工具将工程部署到Scrapyd服务器上面</p>
<p>通常将你的工程部署到Scrapyd需要两个步骤：</p>
<ol>
<li>将工程打包成python蛋，你需要安装<a href="http://pypi.python.org/pypi/setuptools" target="_blank" rel="noopener">setuptools</a></li>
<li>通过<a href="https://scrapyd.readthedocs.org/en/latest/api.html#addversion-json" target="_blank" rel="noopener">addversion.json</a>终端将蟒蛇蛋上传至Scrapd服务器</li>
</ol>
<p>你可以在你的工程配置文件<code>scrapy.cfg</code>定义Scrapyd目标<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[deploy:example]</span><br><span class="line">url = http://scrapyd.example.com/api/scrapyd</span><br><span class="line">username = scrapy</span><br><span class="line">password = secret</span><br></pre></td></tr></table></figure></p>
<p>列出所有可用目标使用命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy -l</span><br></pre></td></tr></table></figure></p>
<p>列出某个目标上面所有可运行的工程，执行命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy -L example</span><br></pre></td></tr></table></figure></p>
<p>先<code>cd</code>到工程根目录，然后使用如下命令来部署：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy &lt;target&gt; -p &lt;project&gt;</span><br></pre></td></tr></table></figure></p>
<p>你还可以定义默认的target和project，省的你每次都去敲代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[deploy]</span><br><span class="line">url = http://scrapyd.example.com/api/scrapyd</span><br><span class="line">username = scrapy</span><br><span class="line">password = secret</span><br><span class="line">project = yourproject</span><br></pre></td></tr></table></figure></p>
<p>这样你就直接取执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy</span><br></pre></td></tr></table></figure></p>
<p>如果你有多个target，那么可以使用下面命令将project部署到多个target服务器上面<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy -a -p &lt;project&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="部署到Scrapy-Cloud"><a href="#部署到Scrapy-Cloud" class="headerlink" title="部署到Scrapy Cloud"></a>部署到Scrapy Cloud</h2><p><a href="http://scrapinghub.com/scrapy-cloud/" target="_blank" rel="noopener">Scrapy Cloud</a>是一个托管的云服务器，由Scrapy背后的公司<a href="http://scrapinghub.com/" target="_blank" rel="noopener">Scrapinghub</a>维护</p>
<p>它免除了安装和监控服务器的需要，并提供了非常美观的UI来管理各个Spider，还能查看被抓取的Item，日志和状态等。</p>
<p>你可以使用<a href="http://doc.scrapinghub.com/shub.html" target="_blank" rel="noopener">shub</a>命令行工具来讲spider部署到Scrapy Cloud。更多请参考<a href="http://doc.scrapinghub.com/scrapy-cloud.html" target="_blank" rel="noopener">官方文档</a></p>
<p>Scrapy Cloud和Scrapyd是兼容的，你可以根据需要在两者之前切换，配置文件也是<code>scrapy.cfg</code>，跟<code>scrapyd-deploy</code>读取的是一样的。</p>
</div></div><a class="button-hover more" href="/2020/01/19/scrapy-部署/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/11/Sqlmap注入使用技巧总结/">Sqlmap注入使用技巧总结</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/笔记总结/">笔记总结</a><i class="fa fa-angle-right" style="margin: 0 8px;"></i><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/笔记总结/软件运用/">软件运用</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/软件/">软件</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/SQL注入/">SQL注入</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Sqlmap/">Sqlmap</a></div></div><div class="post-content"><div class="main-content content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Sqlmap是一种开源的渗透测试工具，可以自动检测和利用SQL注入漏洞以及接入该数据库的服务器。它拥有非常强大的检测引擎、具有多种特性的渗透测试器、通过数据库指纹提取访问底层文件系统并通过外带连接执行命令<br>支持的数据库：</p>
<blockquote>
<p>MySQL, Oracle, PostgreSQL, Microsoft SQL Server, Microsoft Access, IBM DB2, SQLite, Firebird, Sybase and SAP MaxDB </p>
</blockquote>
<p>SQL注入技术：</p>
<blockquote>
<p>boolean-based blind, time-based blind, error-based, UNION query, stacked queries and out-of-band</p>
</blockquote>
<p>枚举数据：</p>
<blockquote>
<p>users, password hashes, privileges, roles, databases, tables and columns</p>
</blockquote></div></div><a class="button-hover more" href="/2020/01/11/Sqlmap注入使用技巧总结/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/10/SQL注入技巧总结/">SQL注入技巧总结</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/笔记总结/">笔记总结</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/SQL注入/">SQL注入</a></div></div><div class="post-content"><div class="main-content content"><h2 id="注入类型总结"><a href="#注入类型总结" class="headerlink" title="注入类型总结"></a>注入类型总结</h2><p>在不断实践中经常遇到如下注入类型，总结如下：</p>
<table>
<thead>
<tr>
<th>注入类型</th>
<th>条件</th>
<th>关键</th>
</tr>
</thead>
<tbody>
<tr>
<td>union注入</td>
<td>显示查询结果</td>
<td>order by , union select</td>
</tr>
<tr>
<td>布尔注入</td>
<td>只返回False或者True</td>
<td>length、ord函数、二分法</td>
</tr>
<tr>
<td>报错注入</td>
<td>返回错误信息</td>
<td>updatexml、group_concat等函数</td>
</tr>
<tr>
<td>时间盲注</td>
<td>无返回信息判断可执行sleep</td>
<td>sleep、if等函数</td>
</tr>
<tr>
<td>宽字节注入</td>
<td>数据库为GBK编码、使用了addslashes函数转义</td>
<td>%df’可以吃掉单引号</td>
</tr>
<tr>
<td>堆叠注入</td>
<td>执行多条查询语句</td>
<td>使用分号分隔多条查询语句</td>
</tr>
<tr>
<td>Cookie注入</td>
<td>在请求头中cookie参数异常</td>
<td>可用 sqlmap -r cook.txt - p 参数</td>
</tr>
<tr>
<td>base64注入</td>
<td>有base64编码过的参数</td>
<td>将payload进行base64编码,可用tamper</td>
</tr>
<tr>
<td>XFF注入</td>
<td>PHP中有getenv函数获取环境配置</td>
<td>x-forward-for、http_client_ip等请求头参数</td>
</tr>
<tr>
<td>二次注入</td>
<td>用户注册功能等</td>
<td>在注册中插入恶意代码，在登录中执行</td>
</tr>
</tbody>
</table></div></div><a class="button-hover more" href="/2020/01/10/SQL注入技巧总结/#more">阅读全文</a></div><div class="recent-post-item"><a class="post-title" href="/2020/01/07/Sqlilabs通关笔记(25-28)绕过注入/">Sqlilabs通关笔记(25-28)绕过注入</a><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 更新于 2020-01-21</time><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/SQL注入/">SQL注入</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/SQL注入/">SQL注入</a></div></div><div class="post-content"><div class="main-content content"><h2 id="第二十五关-基于错误的GET单引号-你的OR及AND归我所有"><a href="#第二十五关-基于错误的GET单引号-你的OR及AND归我所有" class="headerlink" title="第二十五关 基于错误的GET单引号-你的OR及AND归我所有"></a>第二十五关 基于错误的GET单引号-你的OR及AND归我所有</h2><p>这个系列是绕过注入，题目已提示需要绕过的字符，且能显示出输入的payload</p>
<p><img src="https://raw.githubusercontent.com/bbkali/picbad/master/2020-1-8-12-25-30" alt="2020-1-8-12-25-30"></p>
<h3 id="源码审计"><a href="#源码审计" class="headerlink" title="源码审计"></a>源码审计</h3><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">blacklist</span><span class="params">($id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	$id= preg_replace(<span class="string">'/or/i'</span>,<span class="string">""</span>, $id);			<span class="comment">//strip out OR (non case sensitive)</span></span><br><span class="line">	$id= preg_replace(<span class="string">'/AND/i'</span>,<span class="string">""</span>, $id);		<span class="comment">//Strip out AND (non case sensitive)</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">return</span> $id;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>从代码可以看出是吧or和and忽略大小正则替换成空</strong><br></p></div></div><a class="button-hover more" href="/2020/01/07/Sqlilabs通关笔记(25-28)绕过注入/#more">阅读全文</a></div></div><div id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/2/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/4/"><i class="fas fa-angle-right"></i></a></div></div></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fas fa-user"></i></span><span id="busuanzi_value_site_uv"></span><span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fas fa-eye"></i></span><span id="busuanzi_value_site_pv"></span><span></span></div><div class="copyright">&copy;2019 ～ 2020 By bb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>