<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="scrapy-动态配置爬虫"><meta name="keywords" content="Scrapy"><meta name="author" content="bb,shuiyue75381@gmail.com"><meta name="copyright" content="bb"><title>scrapy-动态配置爬虫【bb的博客】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!--link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!--link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!--script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"X7RY6NGU4G","apiKey":"bf7ed65264918bbc03f12b4cc1212d85","indexName":"myblog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy教程10-动态配置爬虫"><span class="toc-number">1.</span> <span class="toc-text">Scrapy教程10- 动态配置爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#脚本运行Scrapy"><span class="toc-number">1.0.1.</span> <span class="toc-text">脚本运行Scrapy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#同一进程运行多个spider"><span class="toc-number">1.0.2.</span> <span class="toc-text">同一进程运行多个spider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义规则表"><span class="toc-number">1.0.3.</span> <span class="toc-text">定义规则表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义文章Item"><span class="toc-number">1.0.4.</span> <span class="toc-text">定义文章Item</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义ArticleSpider"><span class="toc-number">1.0.5.</span> <span class="toc-text">定义ArticleSpider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写pipeline存储到数据库中"><span class="toc-number">1.0.6.</span> <span class="toc-text">编写pipeline存储到数据库中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#修改run-py启动脚本"><span class="toc-number">1.0.7.</span> <span class="toc-text">修改run.py启动脚本</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">bb</div><div class="author-info-description">This is myblog!</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/bbkali" target="_blank">GitHub<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="mailto:shuiyue75381@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color0"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">80</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">65</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">31</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">bb的博客</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">scrapy-动态配置爬虫</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2020-01-19 | 更新于 2020-01-22</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="Scrapy教程10-动态配置爬虫"><a href="#Scrapy教程10-动态配置爬虫" class="headerlink" title="Scrapy教程10- 动态配置爬虫"></a>Scrapy教程10- 动态配置爬虫</h1><p>有很多时候我们需要从多个网站爬取所需要的数据，比如我们想爬取多个网站的新闻，将其存储到数据库同一个表中。我们是不是要对每个网站都得去定义一个Spider类呢？<br>其实不需要，我们可以通过维护一个规则配置表或者一个规则配置文件来动态增加或修改爬取规则，然后程序代码不需要更改就能实现多个网站爬取。</p>
<p>要这样做，我们就不能再使用前面的<code>scrapy crawl test</code>这种命令了，我们需要使用编程的方式运行Scrapy spider，参考<a href="http://doc.scrapy.org/en/1.0/topics/practices.html#run-scrapy-from-a-script" target="_blank" rel="noopener">官方文档</a></p>
<h3 id="脚本运行Scrapy"><a href="#脚本运行Scrapy" class="headerlink" title="脚本运行Scrapy"></a>脚本运行Scrapy</h3><p>可以利用scrapy提供的<a href="http://doc.scrapy.org/en/1.0/topics/api.html#topics-api" target="_blank" rel="noopener">核心API</a>通过编程方式启动scrapy，代替传统的<code>scrapy crawl</code>启动方式。</p>
<p>Scrapy构建于Twisted异步网络框架基础之上，因此你需要在Twisted reactor里面运行。</p>
<p>首先你可以使用<code>scrapy.crawler.CrawlerProcess</code>这个类来运行你的spider，这个类会为你启动一个Twisted reactor，并能配置你的日志和shutdown处理器。所有的scrapy命令都使用这个类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerProcess</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"></span><br><span class="line">process = CrawlerProcess(get_project_settings())</span><br><span class="line"></span><br><span class="line">process.crawl(MySpider)</span><br><span class="line">process.start() <span class="comment"># the script will block here until the crawling is finished</span></span><br></pre></td></tr></table></figure></p>
<p>然后你就可以直接执行这个脚本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run.py</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>另外一个功能更强大的类是<code>scrapy.crawler.CrawlerRunner</code>，推荐你使用这个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">configure_logging(&#123;<span class="string">'LOG_FORMAT'</span>: <span class="string">'%(levelname)s: %(message)s'</span>&#125;)</span><br><span class="line">runner = CrawlerRunner()</span><br><span class="line"></span><br><span class="line">d = runner.crawl(MySpider)</span><br><span class="line">d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line">reactor.run() <span class="comment"># the script will block here until the crawling is finished</span></span><br></pre></td></tr></table></figure></p>
<h3 id="同一进程运行多个spider"><a href="#同一进程运行多个spider" class="headerlink" title="同一进程运行多个spider"></a>同一进程运行多个spider</h3><p>默认情况当你每次执行<code>scrapy crawl</code>命令时会创建一个新的进程。但我们可以使用核心API在同一个进程中同时运行多个spider<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider1</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your first spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider2</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    <span class="comment"># Your second spider definition</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">configure_logging()</span><br><span class="line">runner = CrawlerRunner()</span><br><span class="line">runner.crawl(MySpider1)</span><br><span class="line">runner.crawl(MySpider2)</span><br><span class="line">d = runner.join()</span><br><span class="line">d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line"></span><br><span class="line">reactor.run() <span class="comment"># the script will block here until all crawling jobs are finished</span></span><br></pre></td></tr></table></figure></p>
<h3 id="定义规则表"><a href="#定义规则表" class="headerlink" title="定义规则表"></a>定义规则表</h3><p>好了言归正传，有了前面的脚本启动基础，就可以开始我们的动态配置爬虫了。<br>我们的需求是这样的，从两个不同的网站爬取我们所需要的新闻文章，然后存储到article表中。</p>
<p>首先我们需要定义规则表和文章表，通过动态的创建蜘蛛类，我们以后就只需要维护规则表即可了。这里我使用SQLAlchemy框架来映射数据库。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Topic: 定义数据库模型实体</span></span><br><span class="line"><span class="string">Desc :</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sqlalchemy.engine.url <span class="keyword">import</span> URL</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.ext.declarative <span class="keyword">import</span> declarative_base</span><br><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine, Column, Integer, String, Text, DateTime</span><br><span class="line"><span class="keyword">from</span> coolscrapy.settings <span class="keyword">import</span> DATABASE</span><br><span class="line"></span><br><span class="line">Base = declarative_base()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleRule</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""自定义文章爬取规则"""</span></span><br><span class="line">    __tablename__ = <span class="string">'article_rule'</span></span><br><span class="line"></span><br><span class="line">    id = Column(Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 规则名称</span></span><br><span class="line">    name = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 运行的域名列表，逗号隔开</span></span><br><span class="line">    allow_domains = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 开始URL列表，逗号隔开</span></span><br><span class="line">    start_urls = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 下一页的xpath</span></span><br><span class="line">    next_page = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 文章链接正则表达式(子串)</span></span><br><span class="line">    allow_url = Column(String(<span class="number">200</span>))</span><br><span class="line">    <span class="comment"># 文章链接提取区域xpath</span></span><br><span class="line">    extract_from = Column(String(<span class="number">200</span>))</span><br><span class="line">    <span class="comment"># 文章标题xpath</span></span><br><span class="line">    title_xpath = Column(String(<span class="number">100</span>))</span><br><span class="line">    <span class="comment"># 文章内容xpath</span></span><br><span class="line">    body_xpath = Column(Text)</span><br><span class="line">    <span class="comment"># 发布时间xpath</span></span><br><span class="line">    publish_time_xpath = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 文章来源</span></span><br><span class="line">    source_site = Column(String(<span class="number">30</span>))</span><br><span class="line">    <span class="comment"># 规则是否生效</span></span><br><span class="line">    enable = Column(Integer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Article</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="string">"""文章类"""</span></span><br><span class="line">    __tablename__ = <span class="string">'articles'</span></span><br><span class="line"></span><br><span class="line">    id = Column(Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    url = Column(String(<span class="number">100</span>))</span><br><span class="line">    title = Column(String(<span class="number">100</span>))</span><br><span class="line">    body = Column(Text)</span><br><span class="line">    publish_time = Column(String(<span class="number">30</span>))</span><br><span class="line">    source_site = Column(String(<span class="number">30</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="定义文章Item"><a href="#定义文章Item" class="headerlink" title="定义文章Item"></a>定义文章Item</h3><p>这个很简单了，没什么需要说明的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Article</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    body = scrapy.Field()</span><br><span class="line">    publish_time = scrapy.Field()</span><br><span class="line">    source_site = scrapy.Field()</span><br></pre></td></tr></table></figure></p>
<h3 id="定义ArticleSpider"><a href="#定义ArticleSpider" class="headerlink" title="定义ArticleSpider"></a>定义ArticleSpider</h3><p>接下来我们将定义爬取文章的蜘蛛，这个spider会使用一个Rule实例来初始化，然后根据Rule实例中的xpath规则来获取相应的数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.utils <span class="keyword">import</span> parse_text</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> Article</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">"article"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rule)</span>:</span></span><br><span class="line">        self.rule = rule</span><br><span class="line">        self.name = rule.name</span><br><span class="line">        self.allowed_domains = rule.allow_domains.split(<span class="string">","</span>)</span><br><span class="line">        self.start_urls = rule.start_urls.split(<span class="string">","</span>)</span><br><span class="line">        rule_list = []</span><br><span class="line">        <span class="comment"># 添加`下一页`的规则</span></span><br><span class="line">        <span class="keyword">if</span> rule.next_page:</span><br><span class="line">            rule_list.append(Rule(LinkExtractor(restrict_xpaths=rule.next_page)))</span><br><span class="line">        <span class="comment"># 添加抽取文章链接的规则</span></span><br><span class="line">        rule_list.append(Rule(LinkExtractor(</span><br><span class="line">            allow=[rule.allow_url],</span><br><span class="line">            restrict_xpaths=[rule.extract_from]),</span><br><span class="line">            callback=<span class="string">'parse_item'</span>))</span><br><span class="line">        self.rules = tuple(rule_list)</span><br><span class="line">        super(ArticleSpider, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">'Hi, this is an article page! %s'</span> % response.url)</span><br><span class="line"></span><br><span class="line">        article = Article()</span><br><span class="line">        article[<span class="string">"url"</span>] = response.url</span><br><span class="line"></span><br><span class="line">        title = response.xpath(self.rule.title_xpath).extract()</span><br><span class="line">        article[<span class="string">"title"</span>] = parse_text(title, self.rule.name, <span class="string">'title'</span>)</span><br><span class="line"></span><br><span class="line">        body = response.xpath(self.rule.body_xpath).extract()</span><br><span class="line">        article[<span class="string">"body"</span>] = parse_text(body, self.rule.name, <span class="string">'body'</span>)</span><br><span class="line"></span><br><span class="line">        publish_time = response.xpath(self.rule.publish_time_xpath).extract()</span><br><span class="line">        article[<span class="string">"publish_time"</span>] = parse_text(publish_time, self.rule.name, <span class="string">'publish_time'</span>)</span><br><span class="line"></span><br><span class="line">        article[<span class="string">"source_site"</span>] = self.rule.source_site</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> article</span><br></pre></td></tr></table></figure></p>
<p>要注意的是start_urls，rules等都初始化成了对象的属性，都由传入的rule对象初始化，parse_item方法中的抽取规则也都有rule对象提供。</p>
<h3 id="编写pipeline存储到数据库中"><a href="#编写pipeline存储到数据库中" class="headerlink" title="编写pipeline存储到数据库中"></a>编写pipeline存储到数据库中</h3><p>我们还是使用SQLAlchemy来将文章Item数据存储到数据库中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">session_scope</span><span class="params">(Session)</span>:</span></span><br><span class="line">    <span class="string">"""Provide a transactional scope around a series of operations."""</span></span><br><span class="line">    session = Session()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">yield</span> session</span><br><span class="line">        session.commit()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        session.rollback()</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        session.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleDataBasePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""保存文章到数据库"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        engine = db_connect()</span><br><span class="line">        create_news_table(engine)</span><br><span class="line">        self.Session = sessionmaker(bind=engine)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""This method is called when the spider is opened."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        a = Article(url=item[<span class="string">"url"</span>],</span><br><span class="line">                    title=item[<span class="string">"title"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    publish_time=item[<span class="string">"publish_time"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    body=item[<span class="string">"body"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    source_site=item[<span class="string">"source_site"</span>].encode(<span class="string">"utf-8"</span>))</span><br><span class="line">        <span class="keyword">with</span> session_scope(self.Session) <span class="keyword">as</span> session:</span><br><span class="line">            session.add(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<h3 id="修改run-py启动脚本"><a href="#修改run-py启动脚本" class="headerlink" title="修改run.py启动脚本"></a>修改run.py启动脚本</h3><p>我们将上面的run.py稍作修改即可定制我们的文章爬虫启动脚本<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> spiders.article_spider <span class="keyword">import</span> ArticleSpider</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line"><span class="keyword">from</span> scrapy.crawler <span class="keyword">import</span> CrawlerRunner</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.log <span class="keyword">import</span> configure_logging</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> db_connect</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> ArticleRule</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    settings = get_project_settings()</span><br><span class="line">    configure_logging(settings)</span><br><span class="line">    db = db_connect()</span><br><span class="line">    Session = sessionmaker(bind=db)</span><br><span class="line">    session = Session()</span><br><span class="line">    rules = session.query(ArticleRule).filter(ArticleRule.enable == <span class="number">1</span>).all()</span><br><span class="line">    session.close()</span><br><span class="line">    runner = CrawlerRunner(settings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> rule <span class="keyword">in</span> rules:</span><br><span class="line">        <span class="comment"># stop reactor when spider closes</span></span><br><span class="line">        <span class="comment"># runner.signals.connect(spider_closing, signal=signals.spider_closed)</span></span><br><span class="line">        runner.crawl(ArticleSpider, rule=rule)</span><br><span class="line"></span><br><span class="line">    d = runner.join()</span><br><span class="line">    d.addBoth(<span class="keyword">lambda</span> _: reactor.stop())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># blocks process so always keep as the last statement</span></span><br><span class="line">    reactor.run()</span><br><span class="line">    logging.info(<span class="string">'all finished.'</span>)</span><br></pre></td></tr></table></figure></p>
<p>OK，一切搞定。现在我们可以往ArticleRule表中加入成百上千个网站的规则，而不用添加一行代码，就可以对这成百上千个网站进行爬取。<br>当然你完全可以做一个Web前端来完成维护ArticleRule表的任务。当然ArticleRule规则也可以放在除了数据库的任何地方，比如配置文件。</p>
<p>你可以在<a href="https://github.com/yidao620c/core-scrapy" target="_blank" rel="noopener">GitHub</a>上看到本文的完整项目源码。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:shuiyue75381@gmail.com">bb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="http://bbkali.github.io/2020/01/19/scrapy-动态配置爬虫/">http://bbkali.github.io/2020/01/19/scrapy-动态配置爬虫/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://bbkali.github.io">bb的博客</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2020/01/19/scrapy-模拟登录/"><i class="fas fa-angle-left">&nbsp;</i><span>scrapy-模拟登录</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2020/01/19/scrapy-部署/"><span>scrapy-部署</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2019 ～ 2020 By bb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>