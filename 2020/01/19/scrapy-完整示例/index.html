<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="scrapy-完整示例"><meta name="keywords" content="Scrapy"><meta name="author" content="bb,shuiyue75381@gmail.com"><meta name="copyright" content="bb"><title>scrapy-完整示例【bb的博客】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!--link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!--link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!--script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"X7RY6NGU4G","apiKey":"bf7ed65264918bbc03f12b4cc1212d85","indexName":"myblog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Scrapy教程02-完整示例"><span class="toc-number">1.</span> <span class="toc-text">Scrapy教程02- 完整示例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#创建Scrapy工程"><span class="toc-number">1.1.</span> <span class="toc-text">创建Scrapy工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义我们的Item"><span class="toc-number">1.2.</span> <span class="toc-text">定义我们的Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第一个Spider"><span class="toc-number">1.3.</span> <span class="toc-text">第一个Spider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#运行爬虫"><span class="toc-number">1.4.</span> <span class="toc-text">运行爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#处理链接"><span class="toc-number">1.5.</span> <span class="toc-text">处理链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#导出抓取数据"><span class="toc-number">1.6.</span> <span class="toc-text">导出抓取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保存数据到数据库"><span class="toc-number">1.7.</span> <span class="toc-text">保存数据到数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#下一步"><span class="toc-number">1.8.</span> <span class="toc-text">下一步</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">bb</div><div class="author-info-description">This is myblog!</div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/bbkali" target="_blank">GitHub<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="mailto:shuiyue75381@gmail.com" target="_blank">E-Mail<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="tencent://message/?uin=1019593584&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color1"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">79</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">65</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">31</span></a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">bb的博客</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">scrapy-完整示例</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2020-01-19 | 更新于 2020-01-21</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Scrapy/">Scrapy</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="Scrapy教程02-完整示例"><a href="#Scrapy教程02-完整示例" class="headerlink" title="Scrapy教程02- 完整示例"></a>Scrapy教程02- 完整示例</h1><p>这篇文章我们通过一个比较完整的例子来教你使用Scrapy，我选择爬取<a href="http://www.huxiu.com/" target="_blank" rel="noopener">虎嗅网首页</a>的新闻列表。</p>
<p>这里我们将完成如下几个步骤：</p>
<ul>
<li>创建一个新的Scrapy工程</li>
<li>定义你所需要要抽取的Item对象</li>
<li>编写一个spider来爬取某个网站并提取出所有的Item对象</li>
<li>编写一个Item Pipline来存储提取出来的Item对象</li>
</ul>
<p>Scrapy使用Python语言编写，如果你对这门语言还不熟，请先去学习下基本知识。</p>
<h2 id="创建Scrapy工程"><a href="#创建Scrapy工程" class="headerlink" title="创建Scrapy工程"></a>创建Scrapy工程</h2><p>在任何你喜欢的目录执行如下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject coolscrapy</span><br></pre></td></tr></table></figure></p>
<p>将会创建coolscrapy文件夹，其目录结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">coolscrapy/</span><br><span class="line">    scrapy.cfg            # 部署配置文件</span><br><span class="line"></span><br><span class="line">    coolscrapy/           # Python模块，你所有的代码都放这里面</span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          # Item定义文件</span><br><span class="line"></span><br><span class="line">        pipelines.py      # pipelines定义文件</span><br><span class="line"></span><br><span class="line">        settings.py       # 配置文件</span><br><span class="line"></span><br><span class="line">        spiders/          # 所有爬虫spider都放这个文件夹下面</span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></p>
<h2 id="定义我们的Item"><a href="#定义我们的Item" class="headerlink" title="定义我们的Item"></a>定义我们的Item</h2><p>我们通过创建一个scrapy.Item类，并定义它的类型为scrapy.Field的属性，<br>我们准备将虎嗅网新闻列表的名称、链接地址和摘要爬取下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()    <span class="comment"># 标题</span></span><br><span class="line">    link = scrapy.Field()     <span class="comment"># 链接</span></span><br><span class="line">    desc = scrapy.Field()     <span class="comment"># 简述</span></span><br><span class="line">    posttime = scrapy.Field() <span class="comment"># 发布时间</span></span><br></pre></td></tr></table></figure>
<p>也许你觉得定义这个Item有点麻烦，但是定义完之后你可以得到许多好处，这样你就可以使用Scrapy中其他有用的组件和帮助类。</p>
<h2 id="第一个Spider"><a href="#第一个Spider" class="headerlink" title="第一个Spider"></a>第一个Spider</h2><p>蜘蛛就是你定义的一些类，Scrapy使用它们来从一个domain（或domain组）爬取信息。<br>在蜘蛛类中定义了一个初始化的URL下载列表，以及怎样跟踪链接，如何解析页面内容来提取Item。</p>
<p>定义一个Spider，只需继承<code>scrapy.Spider</code>类并定于一些属性：</p>
<ul>
<li>name: Spider名称，必须是唯一的</li>
<li>start_urls: 初始化下载链接URL</li>
<li>parse(): 用来解析下载后的Response对象，该对象也是这个方法的唯一参数。<br>它负责解析返回页面数据并提取出相应的Item（返回Item对象），还有其他合法的链接URL（返回Request对象）。<a id="more"></a>
我们在coolscrapy/spiders文件夹下面新建<code>huxiu_spider.py</code>，内容如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Topic: sample</span></span><br><span class="line"><span class="string">Desc :</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'desc'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h2><p>在根目录执行下面的命令，其中huxiu是你定义的spider名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>如果一切正常，应该可以打印出每一个新闻</p>
<h2 id="处理链接"><a href="#处理链接" class="headerlink" title="处理链接"></a>处理链接</h2><p>如果想继续跟踪每个新闻链接进去，看看它的详细内容的话，那么可以在parse()方法中返回一个Request对象，<br>然后注册一个回调函数来解析新闻详情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># print(item['title'],item['link'],item['desc'])</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_article)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</span><br><span class="line">        item = HuxiuItem()</span><br><span class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'link'</span>] = response.url</span><br><span class="line">        item[<span class="string">'posttime'</span>] = detail.xpath(</span><br><span class="line">            <span class="string">'div[@class="article-author"]/span[@class="article-time"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'posttime'</span>])</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>现在parse只提取感兴趣的链接，然后将链接内容解析交给另外的方法去处理了。<br>你可以基于这个构建更加复杂的爬虫程序了。</p>
<h2 id="导出抓取数据"><a href="#导出抓取数据" class="headerlink" title="导出抓取数据"></a>导出抓取数据</h2><p>最简单的保存抓取数据的方式是使用json格式的文件保存在本地，像下面这样运行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu -o items.json</span><br></pre></td></tr></table></figure></p>
<p>在演示的小系统里面这种方式足够了。不过如果你要构建复杂的爬虫系统，<br>最好自己编写<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="noopener">Item Pipeline</a>。</p>
<h2 id="保存数据到数据库"><a href="#保存数据到数据库" class="headerlink" title="保存数据到数据库"></a>保存数据到数据库</h2><p>上面我们介绍了可以将抓取的Item导出为json格式的文件，不过最常见的做法还是编写Pipeline将其存储到数据库中。我们在<code>coolscrapy/pipelines.py</code>定义<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> News, db_connect, create_news_table, Article</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleDataBasePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""保存文章到数据库"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        engine = db_connect()</span><br><span class="line">        create_news_table(engine)</span><br><span class="line">        self.Session = sessionmaker(bind=engine)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""This method is called when the spider is opened."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        a = Article(url=item[<span class="string">"url"</span>],</span><br><span class="line">                    title=item[<span class="string">"title"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    publish_time=item[<span class="string">"publish_time"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    body=item[<span class="string">"body"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    source_site=item[<span class="string">"source_site"</span>].encode(<span class="string">"utf-8"</span>))</span><br><span class="line">        <span class="keyword">with</span> session_scope(self.Session) <span class="keyword">as</span> session:</span><br><span class="line">            session.add(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>上面我使用了python中的SQLAlchemy来保存数据库，这个是一个非常优秀的ORM库，我写了篇关于它的<a href="http://yidao620c.github.io/2016/03/07/sqlalchemy.html" target="_blank" rel="noopener">入门教程</a>，可以参考下。</p>
<p>然后在<code>setting.py</code>中配置这个Pipeline，还有数据库链接等信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'coolscrapy.pipelines.ArticleDataBasePipeline'</span>: <span class="number">5</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># linux pip install MySQL-python</span></span><br><span class="line">DATABASE = &#123;<span class="string">'drivername'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'host'</span>: <span class="string">'192.168.203.95'</span>,</span><br><span class="line">            <span class="string">'port'</span>: <span class="string">'3306'</span>,</span><br><span class="line">            <span class="string">'username'</span>: <span class="string">'root'</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'database'</span>: <span class="string">'spider'</span>,</span><br><span class="line">            <span class="string">'query'</span>: &#123;<span class="string">'charset'</span>: <span class="string">'utf8'</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>再次运行爬虫<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>那么所有新闻的文章都存储到数据库中去了。</p>
<h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><p>本章只是带你领略了scrapy最基本的功能，还有很多高级特性没有讲到。接下来会通过多个例子向你展示scrapy的其他特性，然后再深入讲述每个特性。</p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:shuiyue75381@gmail.com">bb</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="http://bbkali.github.io/2020/01/19/scrapy-完整示例/">http://bbkali.github.io/2020/01/19/scrapy-完整示例/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://bbkali.github.io">bb的博客</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2020/01/19/scrapy-文件与图片/"><i class="fas fa-angle-left">&nbsp;</i><span>scrapy-文件与图片</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/2020/01/19/scrapy-入门篇/"><span>scrapy-入门篇</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2019 ～ 2020 By bb</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>